{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mask-R-CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/juliuserbach/Semantic-Features/blob/master/Mask_R_CNN_testing.ipynb",
      "authorship_tag": "ABX9TyPh6I+h1qn9SwLGfIj7HGYn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juliuserbach/Semantic-Features/blob/master/Mask_R_CNN_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqs13ramKg_K",
        "colab_type": "code",
        "outputId": "9571d509-26bd-4df5-9b64-85d3720b0e5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/ftaubner/semantic_features_detection.git"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'semantic_features_detection'...\n",
            "remote: Enumerating objects: 148, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/148)\u001b[K\rremote: Counting objects:   1% (2/148)\u001b[K\rremote: Counting objects:   2% (3/148)\u001b[K\rremote: Counting objects:   3% (5/148)\u001b[K\rremote: Counting objects:   4% (6/148)\u001b[K\rremote: Counting objects:   5% (8/148)\u001b[K\rremote: Counting objects:   6% (9/148)\u001b[K\rremote: Counting objects:   7% (11/148)\u001b[K\rremote: Counting objects:   8% (12/148)\u001b[K\rremote: Counting objects:   9% (14/148)\u001b[K\rremote: Counting objects:  10% (15/148)\u001b[K\rremote: Counting objects:  11% (17/148)\u001b[K\rremote: Counting objects:  12% (18/148)\u001b[K\rremote: Counting objects:  13% (20/148)\u001b[K\rremote: Counting objects:  14% (21/148)\u001b[K\rremote: Counting objects:  15% (23/148)\u001b[K\rremote: Counting objects:  16% (24/148)\u001b[K\rremote: Counting objects:  17% (26/148)\u001b[K\rremote: Counting objects:  18% (27/148)\u001b[K\rremote: Counting objects:  19% (29/148)\u001b[K\rremote: Counting objects:  20% (30/148)\u001b[K\rremote: Counting objects:  21% (32/148)\u001b[K\rremote: Counting objects:  22% (33/148)\u001b[K\rremote: Counting objects:  23% (35/148)\u001b[K\rremote: Counting objects:  24% (36/148)\u001b[K\rremote: Counting objects:  25% (37/148)\u001b[K\rremote: Counting objects:  26% (39/148)\u001b[K\rremote: Counting objects:  27% (40/148)\u001b[K\rremote: Counting objects:  28% (42/148)\u001b[K\rremote: Counting objects:  29% (43/148)\u001b[K\rremote: Counting objects:  30% (45/148)\u001b[K\rremote: Counting objects:  31% (46/148)\u001b[K\rremote: Counting objects:  32% (48/148)\u001b[K\rremote: Counting objects:  33% (49/148)\u001b[K\rremote: Counting objects:  34% (51/148)\u001b[K\rremote: Counting objects:  35% (52/148)\u001b[K\rremote: Counting objects:  36% (54/148)\u001b[K\rremote: Counting objects:  37% (55/148)\u001b[K\rremote: Counting objects:  38% (57/148)\u001b[K\rremote: Counting objects:  39% (58/148)\u001b[K\rremote: Counting objects:  40% (60/148)\u001b[K\rremote: Counting objects:  41% (61/148)\u001b[K\rremote: Counting objects:  42% (63/148)\u001b[K\rremote: Counting objects:  43% (64/148)\u001b[K\rremote: Counting objects:  44% (66/148)\u001b[K\rremote: Counting objects:  45% (67/148)\u001b[K\rremote: Counting objects:  46% (69/148)\u001b[K\rremote: Counting objects:  47% (70/148)\u001b[K\rremote: Counting objects:  48% (72/148)\u001b[K\rremote: Counting objects:  49% (73/148)\u001b[K\rremote: Counting objects:  50% (74/148)\u001b[K\rremote: Counting objects:  51% (76/148)\u001b[K\rremote: Counting objects:  52% (77/148)\u001b[K\rremote: Counting objects:  53% (79/148)\u001b[K\rremote: Counting objects:  54% (80/148)\u001b[K\rremote: Counting objects:  55% (82/148)\u001b[K\rremote: Counting objects:  56% (83/148)\u001b[K\rremote: Counting objects:  57% (85/148)\u001b[K\rremote: Counting objects:  58% (86/148)\u001b[K\rremote: Counting objects:  59% (88/148)\u001b[K\rremote: Counting objects:  60% (89/148)\u001b[K\rremote: Counting objects:  61% (91/148)\u001b[K\rremote: Counting objects:  62% (92/148)\u001b[K\rremote: Counting objects:  63% (94/148)\u001b[K\rremote: Counting objects:  64% (95/148)\u001b[K\rremote: Counting objects:  65% (97/148)\u001b[K\rremote: Counting objects:  66% (98/148)\u001b[K\rremote: Counting objects:  67% (100/148)\u001b[K\rremote: Counting objects:  68% (101/148)\u001b[K\rremote: Counting objects:  69% (103/148)\u001b[K\rremote: Counting objects:  70% (104/148)\u001b[K\rremote: Counting objects:  71% (106/148)\u001b[K\rremote: Counting objects:  72% (107/148)\u001b[K\rremote: Counting objects:  73% (109/148)\u001b[K\rremote: Counting objects:  74% (110/148)\u001b[K\rremote: Counting objects:  75% (111/148)\u001b[K\rremote: Counting objects:  76% (113/148)\u001b[K\rremote: Counting objects:  77% (114/148)\u001b[K\rremote: Counting objects:  78% (116/148)\u001b[K\rremote: Counting objects:  79% (117/148)\u001b[K\rremote: Counting objects:  80% (119/148)\u001b[K\rremote: Counting objects:  81% (120/148)\u001b[K\rremote: Counting objects:  82% (122/148)\u001b[K\rremote: Counting objects:  83% (123/148)\u001b[K\rremote: Counting objects:  84% (125/148)\u001b[K\rremote: Counting objects:  85% (126/148)\u001b[K\rremote: Counting objects:  86% (128/148)\u001b[K\rremote: Counting objects:  87% (129/148)\u001b[K\rremote: Counting objects:  88% (131/148)\u001b[K\rremote: Counting objects:  89% (132/148)\u001b[K\rremote: Counting objects:  90% (134/148)\u001b[K\rremote: Counting objects:  91% (135/148)\u001b[K\rremote: Counting objects:  92% (137/148)\u001b[K\rremote: Counting objects:  93% (138/148)\u001b[K\rremote: Counting objects:  94% (140/148)\u001b[K\rremote: Counting objects:  95% (141/148)\u001b[K\rremote: Counting objects:  96% (143/148)\u001b[K\rremote: Counting objects:  97% (144/148)\u001b[K\rremote: Counting objects:  98% (146/148)\u001b[K\rremote: Counting objects:  99% (147/148)\u001b[K\rremote: Counting objects: 100% (148/148)\u001b[K\rremote: Counting objects: 100% (148/148), done.\u001b[K\n",
            "remote: Compressing objects: 100% (114/114), done.\u001b[K\n",
            "remote: Total 1540 (delta 101), reused 61 (delta 34), pack-reused 1392\u001b[K\n",
            "Receiving objects: 100% (1540/1540), 114.36 MiB | 30.15 MiB/s, done.\n",
            "Resolving deltas: 100% (953/953), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMkSQoH1ZGYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!wget --no-check-certificate \"https://onedrive.live.com/download?cid=EA356294C6263A37&resid=EA356294C6263A37%21100024&authkey=ADLJzehr2ENRggw\" -O mapillary_vistas.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrMxy5HqSm-f",
        "colab_type": "code",
        "outputId": "cd5bf1dc-67b6-4d7f-9f2e-5ba4e63dd371",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "!wget --no-check-certificate \"https://onedrive.live.com/download?cid=EA356294C6263A37&resid=EA356294C6263A37%21101634&authkey=ABTiTiHVXMI06vU\"  -O mapillary_prelim.h5"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-22 17:26:22--  https://onedrive.live.com/download?cid=EA356294C6263A37&resid=EA356294C6263A37%21101634&authkey=ABTiTiHVXMI06vU\n",
            "Resolving onedrive.live.com (onedrive.live.com)... 13.107.42.13\n",
            "Connecting to onedrive.live.com (onedrive.live.com)|13.107.42.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://mmhp0a.am.files.1drv.com/y4m6pmkpVZ23KBIRdPuX4PSkfM5oPCvUzv8JcxpU7lyqtJmOUWX0JF7QoL6vOZb2QBmgcvPd75EwsGPw_w73KYpAjs8fkD1pUrLGuAcuP31iYUO9EadunbgJAz9wcZP1QctUX8QE0uS9dTUYcKfi8kfv_u9jlFZI33Xbf0HZIgemzUi5wbVjB8m14ApIyQBpJyJKUe7z7AA3S1_stRqc37VLw/mask_rcnn_mapvistas_0083.h5?download&psid=1 [following]\n",
            "--2020-05-22 17:26:24--  https://mmhp0a.am.files.1drv.com/y4m6pmkpVZ23KBIRdPuX4PSkfM5oPCvUzv8JcxpU7lyqtJmOUWX0JF7QoL6vOZb2QBmgcvPd75EwsGPw_w73KYpAjs8fkD1pUrLGuAcuP31iYUO9EadunbgJAz9wcZP1QctUX8QE0uS9dTUYcKfi8kfv_u9jlFZI33Xbf0HZIgemzUi5wbVjB8m14ApIyQBpJyJKUe7z7AA3S1_stRqc37VLw/mask_rcnn_mapvistas_0083.h5?download&psid=1\n",
            "Resolving mmhp0a.am.files.1drv.com (mmhp0a.am.files.1drv.com)... 13.107.42.12\n",
            "Connecting to mmhp0a.am.files.1drv.com (mmhp0a.am.files.1drv.com)|13.107.42.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 256204312 (244M) [application/octet-stream]\n",
            "Saving to: ‘mapillary_prelim.h5’\n",
            "\n",
            "mapillary_prelim.h5 100%[===================>] 244.33M  19.3MB/s    in 13s     \n",
            "\n",
            "2020-05-22 17:26:37 (19.5 MB/s) - ‘mapillary_prelim.h5’ saved [256204312/256204312]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nhJZTIzZpRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!unzip -qq /content/mapillary_vistas.zip 'mapillary_vistas/validation/*' -d /content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoNjvU1NcD-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!unzip -qq /content/mapillary_vistas.zip 'mapillary_vistas/config.json' -d /content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdvT1vzBK6hV",
        "colab_type": "code",
        "outputId": "515e629d-c491-48bf-b678-0ec4d2ab0f78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "!pip install imgaug\n",
        "!pip install Cython\n",
        "!pip install pycocotools"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.6/dist-packages (0.2.9)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug) (0.16.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from imgaug) (4.1.2.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.4.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from imgaug) (7.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from imgaug) (3.2.1)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.7.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from imgaug) (2.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.18.4)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (2.4)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (1.1.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug) (0.10.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug) (4.4.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (0.29.18)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.6/dist-packages (2.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meGMIONOKySt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a86c6f06-e9e3-4db1-9f19-2bc5a17509da"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/semantic_features_detection/samples/mapillary/cytools')\n",
        "!python setup.py build_ext --inplace\n",
        "os.chdir('/content/semantic_features_detection/Notebook')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running build_ext\n",
            "copying build/lib.linux-x86_64-3.6/mask_tools.cpython-36m-x86_64-linux-gnu.so -> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viP6_e5JPNXO",
        "colab_type": "code",
        "outputId": "dfeafa4b-63c2-4ef2-de05-be61bcc61bb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import skimage.io\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import imageio\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Root directory of the project\n",
        "ROOT_DIR = os.path.abspath(\"../\")\n",
        "# Ignore Warnings\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
        "\n",
        "# Import Mask RCNN\n",
        "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
        "print(ROOT_DIR)\n",
        "from mrcnn import utils\n",
        "import mrcnn.model_felix as modellib\n",
        "from mrcnn import visualize\n",
        "# Import COCO config\n",
        "sys.path.append(os.path.join(ROOT_DIR, \"samples/mapillary/\"))  # To find local version\n",
        "import mapillary_felix\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Directory to save logs and trained model\n",
        "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
        "print(MODEL_DIR)\n",
        "\n",
        "# Local path to trained weights file\n",
        "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
        "# Download COCO trained weights from Releases if needed\n",
        "if not os.path.exists(COCO_MODEL_PATH):\n",
        "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
        "\n",
        "# Directory of images to run detection on\n",
        "IMAGE_DIR = os.path.join(ROOT_DIR, \"images\")\n",
        "print('finished')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/semantic_features_detection\n",
            "/content/semantic_features_detection/logs\n",
            "finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXAqClvqXsLK",
        "colab_type": "code",
        "outputId": "b8bfbd3a-8662-4a07-a7d4-71471dc564b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "class_names= ['Unlabeled', 'Bird', 'Ground Animal', 'Curb', 'Fence', 'Guard Rail', \n",
        "              'Barrier', 'Wall', 'Bike Lane', 'Crosswalk - Plain', 'Curb Cut', 'Parking', \n",
        "              'Pedestrian Area', 'Rail Track', 'Road', 'Service Lane', 'Sidewalk', 'Bridge', \n",
        "              'Building', 'Tunnel', 'Person', 'Bicyclist', 'Motorcyclist', 'Other Rider', \n",
        "              'Lane Marking - Crosswalk', 'Lane Marking - General', 'Mountain', 'Sand', \n",
        "              'Sky', 'Snow', 'Terrain', 'Vegetation', 'Water', 'Banner', 'Bench', 'Bike Rack',\n",
        "              'Billboard', 'Catch Basin', 'CCTV Camera', 'Fire Hydrant', 'Junction Box', 'Mailbox',\n",
        "              'Manhole', 'Phone Booth', 'Pothole', 'Street Light', 'Pole', 'Traffic Sign Frame', \n",
        "              'Utility Pole', 'Traffic Light', 'Traffic Sign (Back)', 'Traffic Sign (Front)',\n",
        "              'Trash Can', 'Bicycle', 'Boat', 'Bus', 'Car', 'Caravan', 'Motorcycle', 'On Rails',\n",
        "              'Other Vehicle', 'Trailer', 'Truck', 'Wheeled Slow', 'Car Mount', 'Ego Vehicle']\n",
        "selected_classes = [34, 36, 37, 38, 39, 40, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52]\n",
        "\n",
        "print(selected_classes)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[34, 36, 37, 38, 39, 40, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVW7e7R1UWAZ",
        "colab_type": "code",
        "outputId": "d267eff1-a176-4347-be0b-a561259aef59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "class InferenceConfig(mapillary_felix.mapvistas):\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "    DETECTION_MIN_CONFIDENCE = 0\n",
        "    NUM_CLASSES = len(selected_classes) + 1\n",
        "config = InferenceConfig()\n",
        "config.display()\n",
        "batch_size = config.BATCH_SIZE"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Configurations:\n",
            "BACKBONE                       resnet101\n",
            "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
            "BATCH_SIZE                     1\n",
            "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
            "COMPUTE_BACKBONE_SHAPE         None\n",
            "DETECTION_MAX_INSTANCES        100\n",
            "DETECTION_MIN_CONFIDENCE       0\n",
            "DETECTION_NMS_THRESHOLD        0.3\n",
            "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
            "GPU_COUNT                      1\n",
            "GRADIENT_CLIP_NORM             5.0\n",
            "IMAGES_PER_GPU                 1\n",
            "IMAGE_CHANNEL_COUNT            3\n",
            "IMAGE_MAX_DIM                  640\n",
            "IMAGE_META_SIZE                30\n",
            "IMAGE_MIN_DIM                  640\n",
            "IMAGE_MIN_SCALE                0\n",
            "IMAGE_RESIZE_MODE              square\n",
            "IMAGE_SHAPE                    [640 640   3]\n",
            "LEARNING_MOMENTUM              0.9\n",
            "LEARNING_RATE                  0.001\n",
            "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
            "MASK_POOL_SIZE                 14\n",
            "MASK_SHAPE                     [28, 28]\n",
            "MAX_GT_INSTANCES               100\n",
            "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
            "MINI_MASK_SHAPE                (56, 56)\n",
            "NAME                           mapvistas\n",
            "NUM_CLASSES                    18\n",
            "POOL_SIZE                      7\n",
            "POST_NMS_ROIS_INFERENCE        1000\n",
            "POST_NMS_ROIS_TRAINING         2000\n",
            "PRE_NMS_LIMIT                  6000\n",
            "ROI_POSITIVE_RATIO             0.33\n",
            "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
            "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
            "RPN_ANCHOR_STRIDE              1\n",
            "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
            "RPN_NMS_THRESHOLD              0.7\n",
            "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
            "STEPS_PER_EPOCH                1000\n",
            "TOP_DOWN_PYRAMID_SIZE          256\n",
            "TRAIN_BN                       False\n",
            "TRAIN_ROIS_PER_IMAGE           200\n",
            "USE_MINI_MASK                  True\n",
            "USE_RPN_ROIS                   True\n",
            "VALIDATION_STEPS               50\n",
            "WEIGHT_DECAY                   0.0001\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHcm09WTUa4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TRAIN_DIR = '/content/mapillary_vistas'\n",
        "# # Training dataset\n",
        "# dataset_train = mapillary.MapillaryDataset()\n",
        "# dataset_train.load_vistas(dataset_dir=TRAIN_DIR, subset='training', class_ids=selected_classes)\n",
        "# dataset_train.prepare()\n",
        "\n",
        "# Validation dataset\n",
        "VAL_DIR = '/content/drive/My Drive/mapillary_vistas/mapillary_vistas'\n",
        "\n",
        "dataset_val = mapillary_felix.MapillaryDataset()\n",
        "dataset_val.load_vistas(dataset_dir=VAL_DIR, subset='validation', config=config, class_ids=selected_classes)\n",
        "dataset_val.prepare()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P81V1BRHUYu1",
        "colab_type": "code",
        "outputId": "7701befe-6e4a-4149-f0dc-53fbede1f63e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "# Create model in training mode\n",
        "model = modellib.MaskRCNN(mode=\"inference\", config=config,\n",
        "                          model_dir=MODEL_DIR)\n",
        "model_path = '/content/mapillary_prelim.h5'\n",
        "model.load_weights(model_path, by_name=True)\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/semantic_features_detection/mrcnn/model_felix.py:341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/semantic_features_detection/mrcnn/model_felix.py:399: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/semantic_features_detection/mrcnn/model_felix.py:423: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "box_ind is deprecated, use box_indices instead\n",
            "WARNING:tensorflow:From /content/semantic_features_detection/mrcnn/model_felix.py:758: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/semantic_features_detection/mrcnn/model_felix.py:760: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/semantic_features_detection/mrcnn/model_felix.py:772: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz7MKxWOm9JY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def compute_batch_ap(image_ids):\n",
        "#     APs = []\n",
        "#     for image_id in image_ids:\n",
        "#         # Load image\n",
        "#         image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "#             modellib.load_image_gt(dataset_val, config,\n",
        "#                                    image_id, use_mini_mask=False)\n",
        "#         # Run object detection\n",
        "#         results = model.detect([image], verbose=0)\n",
        "#         # Compute AP\n",
        "#         r = results[0]\n",
        "#         AP, precisions, recalls, overlaps =\\\n",
        "#             utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
        "#                               r['rois'], r['class_ids'], r['scores'], r['masks'])\n",
        "#         APs.append(AP)\n",
        "#     return APs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ1Rt7ClnBa8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# image_ids = np.random.choice(dataset_val.image_ids, 300)\n",
        "# APs = compute_batch_ap(image_ids)\n",
        "# print(\"mAP @ IoU=50: \", np.mean(APs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iVmvf40nDek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_batch_ap_classwise(image_ids):\n",
        "    mAPs = np.zeros([len(selected_classes)])\n",
        "    numAPs = np.zeros([len(selected_classes)])\n",
        "    for image_id in image_ids:\n",
        "        # Load image\n",
        "        image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "            modellib.load_image_gt(dataset_val, config,\n",
        "                                  image_id, use_mini_mask=False)\n",
        "        # Run object detection\n",
        "        results = model.detect([image], verbose=0)\n",
        "        # Compute AP\n",
        "        r = results[0]\n",
        "        # visualize.display_instances(image, boxes=r['rois'], masks=r['masks'], class_ids=r['class_ids'], class_names=dataset_val.class_names[1:], scores=r['scores'])\n",
        "        # visualize.display_instances(image, boxes=gt_bbox, masks=gt_mask, class_ids=gt_class_id, class_names=dataset_val.class_names[1:])\n",
        "\n",
        "        for i, class_id in enumerate(range(17)):\n",
        "            # Select one class\n",
        "            mask = np.where(gt_class_id == class_id, True, False)\n",
        "            gt_class_id_sel = gt_class_id[mask]\n",
        "\n",
        "            gt_bbox_sel = gt_bbox[mask,:]\n",
        "            gt_mask_sel = gt_mask[:,:,mask]\n",
        "                            \n",
        "            # Select one class\n",
        "            mask = np.where(r['class_ids'] == class_id, True, False)\n",
        "            r_rois_sel = r['rois'][mask,:]\n",
        "            r_class_ids_sel = r['class_ids'][mask]\n",
        "            r_scores_sel = r['scores'][mask]\n",
        "            r_masks_sel = r['masks'][:,:,mask] \n",
        "            if len(gt_class_id_sel) != 0:\n",
        "                AP, precisions, recalls, overlaps =\\\n",
        "                    utils.compute_ap(gt_bbox_sel, gt_class_id_sel, gt_mask_sel,\n",
        "                                      r_rois_sel, r_class_ids_sel, r_scores_sel, r_masks_sel)\n",
        "                print(\"precisions: {}\".format(precisions))\n",
        "                mAPs[i] = mAPs[i] + AP\n",
        "                # print(\"mAPs: {}\".format(mAPs))\n",
        "                numAPs[i] += 1\n",
        "                # print(\"numAPs: {}\".format(numAPs))\n",
        "\n",
        "    # print(\"mAPs: {}\".format(mAPs))\n",
        "    # print(\"numAPs: {}\".format(numAPs))\n",
        "    mAPs = mAPs / numAPs\n",
        "    return mAPs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzxXBPn-liLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_ids = np.random.choice(dataset_val.image_ids,2)\n",
        "# image_ids = [50]\n",
        "APs = compute_batch_ap_classwise(image_ids)\n",
        "print(\"mAP @ IoU=50: \", APs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itQiOK_GOR0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_batch_ap_classwise_thresh(image_ids, class_id, thresh):\n",
        "    mPs = 0\n",
        "    numPs = 0\n",
        "    for image_id in image_ids:\n",
        "        # Load image\n",
        "        image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "            modellib.load_image_gt(dataset_val, config,\n",
        "                                  image_id, use_mini_mask=False)\n",
        "        # Run object detection\n",
        "        results = model.detect([image], verbose=0)\n",
        "        # Compute AP\n",
        "        r = results[0]\n",
        "        # visualize.display_instances(image, boxes=r['rois'], masks=r['masks'], class_ids=r['class_ids'], class_names=dataset_val.class_names[1:], scores=r['scores'])\n",
        "        # visualize.display_instances(image, boxes=gt_bbox, masks=gt_mask, class_ids=gt_class_id, class_names=dataset_val.class_names[1:])\n",
        "\n",
        "    \n",
        "        # Select one class\n",
        "        mask = np.where(gt_class_id == class_id, True, False)\n",
        "        gt_class_id_sel = gt_class_id[mask]\n",
        "\n",
        "        gt_bbox_sel = gt_bbox[mask,:]\n",
        "        gt_mask_sel = gt_mask[:,:,mask]\n",
        "                        \n",
        "        # Select class and scores higher than threshold\n",
        "        mask = np.where(np.logical_and(r['class_ids'] == class_id, r['scores'] > thresh), True, False)\n",
        "        r_rois_sel = r['rois'][mask,:]\n",
        "        r_class_ids_sel = r['class_ids'][mask]\n",
        "        r_scores_sel = r['scores'][mask]\n",
        "        r_masks_sel = r['masks'][:,:,mask] \n",
        "        if len(r_class_ids_sel) != 0:\n",
        "            AP, precisions, recalls, overlaps =\\\n",
        "                utils.compute_ap(gt_bbox_sel, gt_class_id_sel, gt_mask_sel,\n",
        "                                  r_rois_sel, r_class_ids_sel, r_scores_sel, r_masks_sel)\n",
        "            mPs = mPs + precisions[-2]\n",
        "            numPs += 1\n",
        "\n",
        "    print(\"numPs: {}\".format(numPs))\n",
        "    mPs = mPs / numPs\n",
        "    return mPs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-Y2WsirPlAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mAP_opt = 0\n",
        "thresh_opt = 0\n",
        "class_id = 9\n",
        "image_ids = np.random.choice(dataset_val.image_ids,500)\n",
        "\n",
        "for thresh in range(20):\n",
        "    thresh = thresh/20\n",
        "    mAP = compute_batch_ap_classwise_thresh(image_ids, class_id, thresh)\n",
        "    print(mAP)\n",
        "    print(thresh)\n",
        "    if mAP > mAP_opt:\n",
        "        mAP_opt = mAP\n",
        "        thresh_opt = thresh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9muJiFvwYxKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(mAP_opt)\n",
        "print(thresh_opt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yloSiIQpCYcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SAVE_DIR = '/content/Results'\n",
        "image_ids = np.random.choice(dataset_val.image_ids,4)\n",
        "for image_id in image_ids:\n",
        "    image = dataset_val.load_image(image_id)\n",
        "    results = model.detect([image], verbose=0)\n",
        "    r = results[0]\n",
        "    visualize.save_image(image = image[:,:,::-1], image_name=image_id, boxes=r['rois'], masks=r['masks'], class_ids=r['class_ids'], class_names=class_names, scores=r['scores'], save_dir=SAVE_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGnyg_JypZGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = ['Bench', 'Billboard', 'Catch Basin', 'CCTV Camera', 'Fire Hydrant', 'Junction Box', 'Mailbox',\n",
        "               'Manhole', 'Phone Booth', 'Street Light', 'Pole', 'Traffic Sign Frame', 'Utility Pole',\n",
        "               'Traffic Light', 'Traffic Sign (Back)', 'Traffic Sign (Front)', 'Trash Can']\n",
        "print(dataset_val.class_names[1:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHgMrQYDBpVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(APs)):\n",
        "    print(\"class name: {}: mAP: {}\".format(class_names[i], APs[i]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}