{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mask-R-CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/juliuserbach/Semantic-Features/blob/master/Mask_R_CNN_testing.ipynb",
      "authorship_tag": "ABX9TyMI7Ohyfo42jjW0tYFFi5IR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juliuserbach/Semantic-Features/blob/master/Mask_R_CNN_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqs13ramKg_K",
        "colab_type": "code",
        "outputId": "22eb9211-bc1b-48e4-aa30-fef985ae4cfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/ftaubner/semantic_features_detection.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'semantic_features_detection'...\n",
            "remote: Enumerating objects: 148, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/148)\u001b[K\rremote: Counting objects:   1% (2/148)\u001b[K\rremote: Counting objects:   2% (3/148)\u001b[K\rremote: Counting objects:   3% (5/148)\u001b[K\rremote: Counting objects:   4% (6/148)\u001b[K\rremote: Counting objects:   5% (8/148)\u001b[K\rremote: Counting objects:   6% (9/148)\u001b[K\rremote: Counting objects:   7% (11/148)\u001b[K\rremote: Counting objects:   8% (12/148)\u001b[K\rremote: Counting objects:   9% (14/148)\u001b[K\rremote: Counting objects:  10% (15/148)\u001b[K\rremote: Counting objects:  11% (17/148)\u001b[K\rremote: Counting objects:  12% (18/148)\u001b[K\rremote: Counting objects:  13% (20/148)\u001b[K\rremote: Counting objects:  14% (21/148)\u001b[K\rremote: Counting objects:  15% (23/148)\u001b[K\rremote: Counting objects:  16% (24/148)\u001b[K\rremote: Counting objects:  17% (26/148)\u001b[K\rremote: Counting objects:  18% (27/148)\u001b[K\rremote: Counting objects:  19% (29/148)\u001b[K\rremote: Counting objects:  20% (30/148)\u001b[K\rremote: Counting objects:  21% (32/148)\u001b[K\rremote: Counting objects:  22% (33/148)\u001b[K\rremote: Counting objects:  23% (35/148)\u001b[K\rremote: Counting objects:  24% (36/148)\u001b[K\rremote: Counting objects:  25% (37/148)\u001b[K\rremote: Counting objects:  26% (39/148)\u001b[K\rremote: Counting objects:  27% (40/148)\u001b[K\rremote: Counting objects:  28% (42/148)\u001b[K\rremote: Counting objects:  29% (43/148)\u001b[K\rremote: Counting objects:  30% (45/148)\u001b[K\rremote: Counting objects:  31% (46/148)\u001b[K\rremote: Counting objects:  32% (48/148)\u001b[K\rremote: Counting objects:  33% (49/148)\u001b[K\rremote: Counting objects:  34% (51/148)\u001b[K\rremote: Counting objects:  35% (52/148)\u001b[K\rremote: Counting objects:  36% (54/148)\u001b[K\rremote: Counting objects:  37% (55/148)\u001b[K\rremote: Counting objects:  38% (57/148)\u001b[K\rremote: Counting objects:  39% (58/148)\u001b[K\rremote: Counting objects:  40% (60/148)\u001b[K\rremote: Counting objects:  41% (61/148)\u001b[K\rremote: Counting objects:  42% (63/148)\u001b[K\rremote: Counting objects:  43% (64/148)\u001b[K\rremote: Counting objects:  44% (66/148)\u001b[K\rremote: Counting objects:  45% (67/148)\u001b[K\rremote: Counting objects:  46% (69/148)\u001b[K\rremote: Counting objects:  47% (70/148)\u001b[K\rremote: Counting objects:  48% (72/148)\u001b[K\rremote: Counting objects:  49% (73/148)\u001b[K\rremote: Counting objects:  50% (74/148)\u001b[K\rremote: Counting objects:  51% (76/148)\u001b[K\rremote: Counting objects:  52% (77/148)\u001b[K\rremote: Counting objects:  53% (79/148)\u001b[K\rremote: Counting objects:  54% (80/148)\u001b[K\rremote: Counting objects:  55% (82/148)\u001b[K\rremote: Counting objects:  56% (83/148)\u001b[K\rremote: Counting objects:  57% (85/148)\u001b[K\rremote: Counting objects:  58% (86/148)\u001b[K\rremote: Counting objects:  59% (88/148)\u001b[K\rremote: Counting objects:  60% (89/148)\u001b[K\rremote: Counting objects:  61% (91/148)\u001b[K\rremote: Counting objects:  62% (92/148)\u001b[K\rremote: Counting objects:  63% (94/148)\u001b[K\rremote: Counting objects:  64% (95/148)\u001b[K\rremote: Counting objects:  65% (97/148)\u001b[K\rremote: Counting objects:  66% (98/148)\u001b[K\rremote: Counting objects:  67% (100/148)\rremote: Counting objects:  68% (101/148)\u001b[K\rremote: Counting objects:  69% (103/148)\u001b[K\rremote: Counting objects:  70% (104/148)\u001b[K\rremote: Counting objects:  71% (106/148)\u001b[K\rremote: Counting objects:  72% (107/148)\u001b[K\rremote: Counting objects:  73% (109/148)\u001b[K\rremote: Counting objects:  74% (110/148)\u001b[K\rremote: Counting objects:  75% (111/148)\u001b[K\rremote: Counting objects:  76% (113/148)\u001b[K\rremote: Counting objects:  77% (114/148)\u001b[K\rremote: Counting objects:  78% (116/148)\u001b[K\rremote: Counting objects:  79% (117/148)\u001b[K\rremote: Counting objects:  80% (119/148)\u001b[K\rremote: Counting objects:  81% (120/148)\u001b[K\rremote: Counting objects:  82% (122/148)\u001b[K\rremote: Counting objects:  83% (123/148)\u001b[K\rremote: Counting objects:  84% (125/148)\u001b[K\rremote: Counting objects:  85% (126/148)\u001b[K\rremote: Counting objects:  86% (128/148)\u001b[K\rremote: Counting objects:  87% (129/148)\u001b[K\rremote: Counting objects:  88% (131/148)\u001b[K\rremote: Counting objects:  89% (132/148)\u001b[K\rremote: Counting objects:  90% (134/148)\u001b[K\rremote: Counting objects:  91% (135/148)\u001b[K\rremote: Counting objects:  92% (137/148)\u001b[K\rremote: Counting objects:  93% (138/148)\u001b[K\rremote: Counting objects:  94% (140/148)\u001b[K\rremote: Counting objects:  95% (141/148)\u001b[K\rremote: Counting objects:  96% (143/148)\u001b[K\rremote: Counting objects:  97% (144/148)\u001b[K\rremote: Counting objects:  98% (146/148)\u001b[K\rremote: Counting objects:  99% (147/148)\u001b[K\rremote: Counting objects: 100% (148/148)\u001b[K\rremote: Counting objects: 100% (148/148), done.\u001b[K\n",
            "remote: Compressing objects: 100% (114/114), done.\u001b[K\n",
            "remote: Total 1540 (delta 101), reused 61 (delta 34), pack-reused 1392\u001b[K\n",
            "Receiving objects: 100% (1540/1540), 114.36 MiB | 30.90 MiB/s, done.\n",
            "Resolving deltas: 100% (953/953), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMkSQoH1ZGYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!wget --no-check-certificate \"https://onedrive.live.com/download?cid=EA356294C6263A37&resid=EA356294C6263A37%21100024&authkey=ADLJzehr2ENRggw\" -O mapillary_vistas.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrMxy5HqSm-f",
        "colab_type": "code",
        "outputId": "7fd5487d-6dfc-4b3e-ba47-bad967ee9d8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "!wget --no-check-certificate \"https://onedrive.live.com/download?cid=EA356294C6263A37&resid=EA356294C6263A37%21101634&authkey=ABTiTiHVXMI06vU\"  -O mapillary_prelim.h5"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-23 15:56:51--  https://onedrive.live.com/download?cid=EA356294C6263A37&resid=EA356294C6263A37%21101634&authkey=ABTiTiHVXMI06vU\n",
            "Resolving onedrive.live.com (onedrive.live.com)... 13.107.42.13\n",
            "Connecting to onedrive.live.com (onedrive.live.com)|13.107.42.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://mmhp0a.am.files.1drv.com/y4m4DR4l1wu0d_GE-9KJP8Kr_7OLjl8r1UUGwpj8xPnoq_ArnJfb9M-zEKcJt98u9Y-w8uYNte4rr5hkFcnHo3AvaLMv4TmDjMeCd8SVItptHXHWcr7KT0MW-aJ4USEVEwjxYSUBRhQ3kG8FCrO2onLpexSalcZaGJiM9QS-9SRvW92cmUK8opsWoB8tZ1DZMs1YsWCEgwFt2ZZxwrYoYZJmQ/mask_rcnn_mapvistas_0083.h5?download&psid=1 [following]\n",
            "--2020-05-23 15:56:52--  https://mmhp0a.am.files.1drv.com/y4m4DR4l1wu0d_GE-9KJP8Kr_7OLjl8r1UUGwpj8xPnoq_ArnJfb9M-zEKcJt98u9Y-w8uYNte4rr5hkFcnHo3AvaLMv4TmDjMeCd8SVItptHXHWcr7KT0MW-aJ4USEVEwjxYSUBRhQ3kG8FCrO2onLpexSalcZaGJiM9QS-9SRvW92cmUK8opsWoB8tZ1DZMs1YsWCEgwFt2ZZxwrYoYZJmQ/mask_rcnn_mapvistas_0083.h5?download&psid=1\n",
            "Resolving mmhp0a.am.files.1drv.com (mmhp0a.am.files.1drv.com)... 13.107.42.12\n",
            "Connecting to mmhp0a.am.files.1drv.com (mmhp0a.am.files.1drv.com)|13.107.42.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 256204312 (244M) [application/octet-stream]\n",
            "Saving to: ‘mapillary_prelim.h5’\n",
            "\n",
            "mapillary_prelim.h5 100%[===================>] 244.33M  37.7MB/s    in 6.6s    \n",
            "\n",
            "2020-05-23 15:56:59 (37.3 MB/s) - ‘mapillary_prelim.h5’ saved [256204312/256204312]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nhJZTIzZpRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!unzip -qq /content/mapillary_vistas.zip 'mapillary_vistas/validation/*' -d /content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoNjvU1NcD-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!unzip -qq /content/mapillary_vistas.zip 'mapillary_vistas/config.json' -d /content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdvT1vzBK6hV",
        "colab_type": "code",
        "outputId": "93411548-4a9a-4e01-eb52-bf270c7a978e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "!pip install imgaug\n",
        "!pip install Cython\n",
        "!pip install pycocotools"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.6/dist-packages (0.2.9)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug) (0.16.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.18.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from imgaug) (4.1.2.30)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.7.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from imgaug) (3.2.1)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from imgaug) (2.4.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from imgaug) (7.0.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (2.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug) (1.2.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug) (4.4.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (0.29.18)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.6/dist-packages (2.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meGMIONOKySt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "79ecef29-cbf3-48ec-b458-372c87b38229"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/semantic_features_detection/samples/mapillary/cytools')\n",
        "!python setup.py build_ext --inplace\n",
        "os.chdir('/content/semantic_features_detection/Notebook')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running build_ext\n",
            "building 'mask_tools' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.6\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c mask_tools.c -o build/temp.linux-x86_64-3.6/mask_tools.o\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1832:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kmask_tools.c:613\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\n",
            " #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\n",
            "  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "creating build/lib.linux-x86_64-3.6\n",
            "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/mask_tools.o -o build/lib.linux-x86_64-3.6/mask_tools.cpython-36m-x86_64-linux-gnu.so\n",
            "copying build/lib.linux-x86_64-3.6/mask_tools.cpython-36m-x86_64-linux-gnu.so -> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viP6_e5JPNXO",
        "colab_type": "code",
        "outputId": "4693ba11-d44d-44e1-aff7-d435a5c8c962",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import skimage.io\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import imageio\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Root directory of the project\n",
        "ROOT_DIR = os.path.abspath(\"../\")\n",
        "# Ignore Warnings\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
        "\n",
        "# Import Mask RCNN\n",
        "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
        "print(ROOT_DIR)\n",
        "from mrcnn import utils\n",
        "import mrcnn.model_felix as modellib\n",
        "from mrcnn import visualize\n",
        "# Import COCO config\n",
        "sys.path.append(os.path.join(ROOT_DIR, \"samples/mapillary/\"))  # To find local version\n",
        "import mapillary_felix\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Directory to save logs and trained model\n",
        "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
        "print(MODEL_DIR)\n",
        "\n",
        "# Local path to trained weights file\n",
        "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
        "# Download COCO trained weights from Releases if needed\n",
        "if not os.path.exists(COCO_MODEL_PATH):\n",
        "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
        "\n",
        "# Directory of images to run detection on\n",
        "IMAGE_DIR = os.path.join(ROOT_DIR, \"images\")\n",
        "print('finished')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "/content/semantic_features_detection\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/semantic_features_detection/logs\n",
            "Downloading pretrained model to /content/semantic_features_detection/mask_rcnn_coco.h5 ...\n",
            "... done downloading pretrained model!\n",
            "finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXAqClvqXsLK",
        "colab_type": "code",
        "outputId": "32a8b074-26d9-4e7f-944f-d28cffd7d75b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "class_names= ['Unlabeled', 'Bird', 'Ground Animal', 'Curb', 'Fence', 'Guard Rail', \n",
        "              'Barrier', 'Wall', 'Bike Lane', 'Crosswalk - Plain', 'Curb Cut', 'Parking', \n",
        "              'Pedestrian Area', 'Rail Track', 'Road', 'Service Lane', 'Sidewalk', 'Bridge', \n",
        "              'Building', 'Tunnel', 'Person', 'Bicyclist', 'Motorcyclist', 'Other Rider', \n",
        "              'Lane Marking - Crosswalk', 'Lane Marking - General', 'Mountain', 'Sand', \n",
        "              'Sky', 'Snow', 'Terrain', 'Vegetation', 'Water', 'Banner', 'Bench', 'Bike Rack',\n",
        "              'Billboard', 'Catch Basin', 'CCTV Camera', 'Fire Hydrant', 'Junction Box', 'Mailbox',\n",
        "              'Manhole', 'Phone Booth', 'Pothole', 'Street Light', 'Pole', 'Traffic Sign Frame', \n",
        "              'Utility Pole', 'Traffic Light', 'Traffic Sign (Back)', 'Traffic Sign (Front)',\n",
        "              'Trash Can', 'Bicycle', 'Boat', 'Bus', 'Car', 'Caravan', 'Motorcycle', 'On Rails',\n",
        "              'Other Vehicle', 'Trailer', 'Truck', 'Wheeled Slow', 'Car Mount', 'Ego Vehicle']\n",
        "selected_classes = [34, 36, 37, 38, 39, 40, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52]\n",
        "\n",
        "print(selected_classes)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[34, 36, 37, 38, 39, 40, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVW7e7R1UWAZ",
        "colab_type": "code",
        "outputId": "8ab9eda1-07b2-48bb-feec-a45f5eeb0371",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "class InferenceConfig(mapillary_felix.mapvistas):\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "    DETECTION_MIN_CONFIDENCE = 0\n",
        "    NUM_CLASSES = len(selected_classes) + 1\n",
        "config = InferenceConfig()\n",
        "config.display()\n",
        "batch_size = config.BATCH_SIZE"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Configurations:\n",
            "BACKBONE                       resnet101\n",
            "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
            "BATCH_SIZE                     1\n",
            "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
            "COMPUTE_BACKBONE_SHAPE         None\n",
            "DETECTION_MAX_INSTANCES        100\n",
            "DETECTION_MIN_CONFIDENCE       0\n",
            "DETECTION_NMS_THRESHOLD        0.3\n",
            "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
            "GPU_COUNT                      1\n",
            "GRADIENT_CLIP_NORM             5.0\n",
            "IMAGES_PER_GPU                 1\n",
            "IMAGE_CHANNEL_COUNT            3\n",
            "IMAGE_MAX_DIM                  640\n",
            "IMAGE_META_SIZE                30\n",
            "IMAGE_MIN_DIM                  640\n",
            "IMAGE_MIN_SCALE                0\n",
            "IMAGE_RESIZE_MODE              square\n",
            "IMAGE_SHAPE                    [640 640   3]\n",
            "LEARNING_MOMENTUM              0.9\n",
            "LEARNING_RATE                  0.001\n",
            "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
            "MASK_POOL_SIZE                 14\n",
            "MASK_SHAPE                     [28, 28]\n",
            "MAX_GT_INSTANCES               100\n",
            "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
            "MINI_MASK_SHAPE                (56, 56)\n",
            "NAME                           mapvistas\n",
            "NUM_CLASSES                    18\n",
            "POOL_SIZE                      7\n",
            "POST_NMS_ROIS_INFERENCE        1000\n",
            "POST_NMS_ROIS_TRAINING         2000\n",
            "PRE_NMS_LIMIT                  6000\n",
            "ROI_POSITIVE_RATIO             0.33\n",
            "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
            "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
            "RPN_ANCHOR_STRIDE              1\n",
            "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
            "RPN_NMS_THRESHOLD              0.7\n",
            "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
            "STEPS_PER_EPOCH                1000\n",
            "TOP_DOWN_PYRAMID_SIZE          256\n",
            "TRAIN_BN                       False\n",
            "TRAIN_ROIS_PER_IMAGE           200\n",
            "USE_MINI_MASK                  True\n",
            "USE_RPN_ROIS                   True\n",
            "VALIDATION_STEPS               50\n",
            "WEIGHT_DECAY                   0.0001\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHcm09WTUa4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TRAIN_DIR = '/content/mapillary_vistas'\n",
        "# # Training dataset\n",
        "# dataset_train = mapillary.MapillaryDataset()\n",
        "# dataset_train.load_vistas(dataset_dir=TRAIN_DIR, subset='training', class_ids=selected_classes)\n",
        "# dataset_train.prepare()\n",
        "\n",
        "# Validation dataset\n",
        "VAL_DIR = '/content/drive/My Drive/mapillary_vistas/mapillary_vistas'\n",
        "\n",
        "dataset_val = mapillary_felix.MapillaryDataset()\n",
        "dataset_val.load_vistas(dataset_dir=VAL_DIR, subset='validation', config=config, class_ids=selected_classes)\n",
        "dataset_val.prepare()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P81V1BRHUYu1",
        "colab_type": "code",
        "outputId": "6c507bcf-bbb6-4785-cbd6-1797ad410451",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "# Create model in training mode\n",
        "model = modellib.MaskRCNN(mode=\"inference\", config=config,\n",
        "                          model_dir=MODEL_DIR)\n",
        "model_path = '/content/mapillary_prelim.h5'\n",
        "model.load_weights(model_path, by_name=True)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/semantic_features_detection/mrcnn/model_felix.py:341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/semantic_features_detection/mrcnn/model_felix.py:399: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/semantic_features_detection/mrcnn/model_felix.py:423: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "box_ind is deprecated, use box_indices instead\n",
            "WARNING:tensorflow:From /content/semantic_features_detection/mrcnn/model_felix.py:758: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/semantic_features_detection/mrcnn/model_felix.py:760: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/semantic_features_detection/mrcnn/model_felix.py:772: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtMGD9UDXzIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_overlaps_masks(masks1, masks2):\n",
        "    \"\"\"Computes IoU overlaps between two sets of masks.\n",
        "    masks1, masks2: [Height, Width, instances]\n",
        "    \"\"\"\n",
        "    \n",
        "    # If either set of masks is empty return empty result\n",
        "    if masks1.shape[-1] == 0 or masks2.shape[-1] == 0:\n",
        "        return np.zeros((masks1.shape[-1], masks2.shape[-1]))\n",
        "    # flatten masks and compute their areas\n",
        "    masks1 = np.reshape(masks1 > .5, (-1, masks1.shape[-1])).astype(np.float32)\n",
        "    masks2 = np.reshape(masks2 > .5, (-1, masks2.shape[-1])).astype(np.float32)\n",
        "    area1 = np.sum(masks1, axis=0)\n",
        "    area2 = np.sum(masks2, axis=0)\n",
        "\n",
        "    # intersections and union\n",
        "    intersections = np.dot(masks1.T, masks2)\n",
        "    union = area1[:, None] + area2[None, :] - intersections\n",
        "    overlaps = intersections / union\n",
        "\n",
        "    return overlaps\n",
        "\n",
        "def trim_zeros(x):\n",
        "    \"\"\"It's common to have tensors larger than the available data and\n",
        "    pad with zeros. This function removes rows that are all zeros.\n",
        "\n",
        "    x: [rows, columns].\n",
        "    \"\"\"\n",
        "    assert len(x.shape) == 2\n",
        "    return x[~np.all(x == 0, axis=1)]\n",
        "\n",
        "\n",
        "def compute_matches(gt_boxes, gt_class_ids, gt_masks,\n",
        "                    pred_boxes, pred_class_ids, pred_scores, pred_masks,\n",
        "                    iou_threshold=0.5, score_threshold=0.0):\n",
        "    \"\"\"Finds matches between prediction and ground truth instances.\n",
        "\n",
        "    Returns:\n",
        "        gt_match: 1-D array. For each GT box it has the index of the matched\n",
        "                  predicted box.\n",
        "        pred_match: 1-D array. For each predicted box, it has the index of\n",
        "                    the matched ground truth box.\n",
        "        overlaps: [pred_boxes, gt_boxes] IoU overlaps.\n",
        "    \"\"\"\n",
        "    # Trim zero padding\n",
        "    # TODO: cleaner to do zero unpadding upstream\n",
        "    gt_boxes = trim_zeros(gt_boxes)\n",
        "    gt_masks = gt_masks[..., :gt_boxes.shape[0]]\n",
        "    pred_boxes = trim_zeros(pred_boxes)\n",
        "    pred_scores = pred_scores[:pred_boxes.shape[0]]\n",
        "    # Sort predictions by score from high to low\n",
        "    indices = np.argsort(pred_scores)[::-1]\n",
        "    pred_boxes = pred_boxes[indices]\n",
        "    pred_class_ids = pred_class_ids[indices]\n",
        "    pred_scores = pred_scores[indices]\n",
        "    pred_masks = pred_masks[..., indices]\n",
        "\n",
        "    # Compute IoU overlaps [pred_masks, gt_masks]\n",
        "    overlaps = compute_overlaps_masks(pred_masks, gt_masks)\n",
        "\n",
        "    # Loop through predictions and find matching ground truth boxes\n",
        "    match_count = 0\n",
        "    pred_match = -1 * np.ones([pred_boxes.shape[0]])\n",
        "    gt_match = -1 * np.ones([gt_boxes.shape[0]])\n",
        "    for i in range(len(pred_boxes)):\n",
        "        # Find best matching ground truth box\n",
        "        # 1. Sort matches by score\n",
        "        sorted_ixs = np.argsort(overlaps[i])[::-1]\n",
        "        # 2. Remove low scores\n",
        "        low_score_idx = np.where(overlaps[i, sorted_ixs] < score_threshold)[0]\n",
        "        if low_score_idx.size > 0:\n",
        "            sorted_ixs = sorted_ixs[:low_score_idx[0]]\n",
        "        # 3. Find the match\n",
        "        for j in sorted_ixs:\n",
        "            # If ground truth box is already matched, go to next one\n",
        "            if gt_match[j] > -1:\n",
        "                continue\n",
        "            # If we reach IoU smaller than the threshold, end the loop\n",
        "            iou = overlaps[i, j]\n",
        "            if iou < iou_threshold:\n",
        "                break\n",
        "            # Do we have a match?\n",
        "            \n",
        "            match_count += 1\n",
        "            gt_match[j] = i\n",
        "            pred_match[i] = j\n",
        "            break\n",
        "\n",
        "    return gt_match, pred_match, overlaps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0RZ4wxeQbBu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "907b6652-917c-453a-d410-09df7187389b"
      },
      "source": [
        "image_ids = np.random.choice(dataset_val.image_ids,2000)\n",
        "class_PMF = np.zeros((len(selected_classes), (len(selected_classes) + 1)))\n",
        "for image_id in image_ids:\n",
        "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "          modellib.load_image_gt(dataset_val, config,\n",
        "                                image_id, use_mini_mask=False)\n",
        "    results = model.detect([image], verbose=0)\n",
        "    r = results[0]\n",
        "    #visualize.display_instances(image, boxes=r['rois'], masks=r['masks'], class_ids=r['class_ids'], class_names=dataset_val.class_names[1:], scores=r['scores'])\n",
        "    gt_match, pred_match, overlaps = compute_matches(gt_boxes=gt_bbox, gt_class_ids=gt_class_id, gt_masks=gt_mask, pred_boxes=r['rois'], \n",
        "                          pred_class_ids=r['class_ids'], pred_scores=r['scores'], pred_masks=r['masks'], iou_threshold=0.5)\n",
        "    \n",
        "    gt_match = list(map(int, gt_match))\n",
        "    for i, matched_pred in enumerate(gt_match):\n",
        "        if matched_pred == -1:  \n",
        "            class_PMF[gt_class_id[i]-1, 0] += 1 \n",
        "        else:\n",
        "            class_PMF[gt_class_id[i]-1, r['class_ids'][matched_pred]] += 1 \n",
        "for i, class_Pr in enumerate(class_PMF):\n",
        "    summed = class_Pr.sum()\n",
        "    if summed > 0:\n",
        "        class_PMF[i,:] = class_PMF[i,:]/summed\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:You are using the default load_mask(), maybe you need to define your own one.\n",
            "WARNING:root:You are using the default load_mask(), maybe you need to define your own one.\n",
            "WARNING:root:You are using the default load_mask(), maybe you need to define your own one.\n",
            "WARNING:root:You are using the default load_mask(), maybe you need to define your own one.\n",
            "WARNING:root:You are using the default load_mask(), maybe you need to define your own one.\n",
            "WARNING:root:You are using the default load_mask(), maybe you need to define your own one.\n",
            "WARNING:root:You are using the default load_mask(), maybe you need to define your own one.\n",
            "WARNING:root:You are using the default load_mask(), maybe you need to define your own one.\n",
            "WARNING:root:You are using the default load_mask(), maybe you need to define your own one.\n",
            "WARNING:root:You are using the default load_mask(), maybe you need to define your own one.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hboqg2TzpNSD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "cac14c61-75e8-46f7-c59b-e5a3e1339210"
      },
      "source": [
        "PMF_dict = {}\n",
        "for i, class_Pr in enumerate(class_PMF):\n",
        "    PMF_dict[dataset_val.class_names[i+1]] = list(class_Pr)\n",
        "print(PMF_dict)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Bench': [0.5652173913043478, 0.34441156385158206, 0.0, 0.0, 0.0, 0.0006829046209879354, 0.0, 0.0, 0.0, 0.0015934441156385158, 0.0011381743683132257, 0.00022763487366264513, 0.00045526974732529026, 0.005918506715228773, 0.01024356931481903, 0.06692465285681766, 0.0031868882312770315, 0.0], 'Billboard': [0.7125748502994012, 0.0, 0.2155688622754491, 0.0, 0.0, 0.0, 0.0, 0.0718562874251497, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'Catch Basin': [0.7368421052631579, 0.0, 0.0, 0.10526315789473684, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15789473684210525, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'CCTV Camera': [0.29508196721311475, 0.0, 0.0, 0.0, 0.4918032786885246, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14754098360655737, 0.0, 0.0, 0.0, 0.0, 0.06557377049180328, 0.0, 0.0], 'Fire Hydrant': [0.5948717948717949, 0.06666666666666667, 0.0, 0.002564102564102564, 0.005128205128205128, 0.24102564102564103, 0.0, 0.0, 0.0, 0.002564102564102564, 0.010256410256410256, 0.0, 0.0, 0.005128205128205128, 0.03076923076923077, 0.015384615384615385, 0.02564102564102564, 0.0], 'Junction Box': [0.6129032258064516, 0.06451612903225806, 0.0, 0.0, 0.06451612903225806, 0.12903225806451613, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03225806451612903, 0.03225806451612903, 0.06451612903225806, 0.0], 'Mailbox': [0.6176470588235294, 0.0, 0.02249134948096886, 0.0, 0.0, 0.0, 0.0, 0.35986159169550175, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'Manhole': [0.7419354838709677, 0.03225806451612903, 0.0, 0.0, 0.0, 0.06451612903225806, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03225806451612903, 0.12903225806451613, 0.0], 'Phone Booth': [0.3667992047713718, 0.0, 0.0, 0.005964214711729622, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6083499005964215, 0.0, 0.0, 0.0, 0.007952286282306162, 0.003976143141153081, 0.006958250497017893, 0.0, 0.0], 'Street Light': [0.6554727793696276, 0.00011461318051575932, 0.0, 0.0, 0.0005730659025787965, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32011461318051576, 0.0, 0.02177650429799427, 0.0, 0.00045845272206303727, 0.0010315186246418338, 0.00045845272206303727, 0.0], 'Pole': [0.9317269076305221, 0.012048192771084338, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0321285140562249, 0.012048192771084338, 0.008032128514056224, 0.0, 0.004016064257028112, 0.0, 0.0, 0.0], 'Traffic Sign Frame': [0.6151505932461211, 0.000608457560085184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0018253726802555522, 0.07788256769090356, 0.0, 0.3045330088226346, 0.0, 0.0, 0.0, 0.0, 0.0], 'Utility Pole': [0.3300567107750473, 0.005293005671077505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0060491493383742915, 0.0, 0.0, 0.0, 0.6283553875236295, 0.01058601134215501, 0.019659735349716444, 0.0, 0.0], 'Traffic Light': [0.561284046692607, 0.02821011673151751, 0.0, 0.0, 0.0009727626459143969, 0.010700389105058366, 0.0, 0.0, 0.0, 0.0009727626459143969, 0.0, 0.0, 0.0, 0.026264591439688716, 0.32782101167315175, 0.04377431906614786, 0.0, 0.0], 'Traffic Sign (Back)': [0.3111523301646186, 0.0744261534894505, 0.0, 0.00023185717597959656, 0.00023185717597959656, 0.0, 0.0, 0.0, 0.0, 0.002550428935775562, 0.0006955715279387897, 0.0, 0.0, 0.013215859030837005, 0.011592858798979828, 0.5859030837004405, 0.0, 0.0], 'Traffic Sign (Front)': [0.6111111111111112, 0.024444444444444446, 0.0, 0.0, 0.008888888888888889, 0.03333333333333333, 0.0, 0.0, 0.0, 0.0, 0.015555555555555555, 0.0, 0.0, 0.008888888888888889, 0.0022222222222222222, 0.015555555555555555, 0.28, 0.0], 'Trash Can': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F93H_fRrpzpK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "with open('/content/PMFs.json', 'w') as outfile:\n",
        "    json.dump(PMF_dict, outfile)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz7MKxWOm9JY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def compute_batch_ap(image_ids):\n",
        "#     APs = []\n",
        "#     for image_id in image_ids:\n",
        "#         # Load image\n",
        "#         image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "#             modellib.load_image_gt(dataset_val, config,\n",
        "#                                    image_id, use_mini_mask=False)\n",
        "#         # Run object detection\n",
        "#         results = model.detect([image], verbose=0)\n",
        "#         # Compute AP\n",
        "#         r = results[0]\n",
        "#         AP, precisions, recalls, overlaps =\\\n",
        "#             utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
        "#                               r['rois'], r['class_ids'], r['scores'], r['masks'])\n",
        "#         APs.append(AP)\n",
        "#     return APs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ1Rt7ClnBa8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# image_ids = np.random.choice(dataset_val.image_ids, 300)\n",
        "# APs = compute_batch_ap(image_ids)\n",
        "# print(\"mAP @ IoU=50: \", np.mean(APs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iVmvf40nDek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_batch_ap_classwise(image_ids):\n",
        "    mAPs = np.zeros([len(selected_classes)])\n",
        "    numAPs = np.zeros([len(selected_classes)])\n",
        "    for image_id in image_ids:\n",
        "        # Load image\n",
        "        image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "            modellib.load_image_gt(dataset_val, config,\n",
        "                                  image_id, use_mini_mask=False)\n",
        "        # Run object detection\n",
        "        results = model.detect([image], verbose=0)\n",
        "        # Compute AP\n",
        "        r = results[0]\n",
        "        # visualize.display_instances(image, boxes=r['rois'], masks=r['masks'], class_ids=r['class_ids'], class_names=dataset_val.class_names[1:], scores=r['scores'])\n",
        "        # visualize.display_instances(image, boxes=gt_bbox, masks=gt_mask, class_ids=gt_class_id, class_names=dataset_val.class_names[1:])\n",
        "\n",
        "        for i, class_id in enumerate(range(17)):\n",
        "            # Select one class\n",
        "            mask = np.where(gt_class_id == class_id, True, False)\n",
        "            gt_class_id_sel = gt_class_id[mask]\n",
        "\n",
        "            gt_bbox_sel = gt_bbox[mask,:]\n",
        "            gt_mask_sel = gt_mask[:,:,mask]\n",
        "                            \n",
        "            # Select one class\n",
        "            mask = np.where(r['class_ids'] == class_id, True, False)\n",
        "            r_rois_sel = r['rois'][mask,:]\n",
        "            r_class_ids_sel = r['class_ids'][mask]\n",
        "            r_scores_sel = r['scores'][mask]\n",
        "            r_masks_sel = r['masks'][:,:,mask] \n",
        "            if len(gt_class_id_sel) != 0:\n",
        "                AP, precisions, recalls, overlaps =\\\n",
        "                    utils.compute_ap(gt_bbox_sel, gt_class_id_sel, gt_mask_sel,\n",
        "                                      r_rois_sel, r_class_ids_sel, r_scores_sel, r_masks_sel)\n",
        "                print(\"precisions: {}\".format(precisions))\n",
        "                mAPs[i] = mAPs[i] + AP\n",
        "                # print(\"mAPs: {}\".format(mAPs))\n",
        "                numAPs[i] += 1\n",
        "                # print(\"numAPs: {}\".format(numAPs))\n",
        "\n",
        "    # print(\"mAPs: {}\".format(mAPs))\n",
        "    # print(\"numAPs: {}\".format(numAPs))\n",
        "    mAPs = mAPs / numAPs\n",
        "    return mAPs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzxXBPn-liLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_ids = np.random.choice(dataset_val.image_ids,2)\n",
        "# image_ids = [50]\n",
        "APs = compute_batch_ap_classwise(image_ids)\n",
        "print(\"mAP @ IoU=50: \", APs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-Y2WsirPlAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mAP_opt = 0\n",
        "thresh_opt = 0\n",
        "class_id = 9\n",
        "image_ids = np.random.choice(dataset_val.image_ids,500)\n",
        "\n",
        "for thresh in range(20):\n",
        "    thresh = thresh/20\n",
        "    mAP = compute_batch_ap_classwise_thresh(image_ids, class_id, thresh)\n",
        "    print(mAP)\n",
        "    print(thresh)\n",
        "    if mAP > mAP_opt:\n",
        "        mAP_opt = mAP\n",
        "        thresh_opt = thresh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9muJiFvwYxKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(mAP_opt)\n",
        "print(thresh_opt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yloSiIQpCYcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SAVE_DIR = '/content/Results'\n",
        "image_ids = np.random.choice(dataset_val.image_ids,4)\n",
        "for image_id in image_ids:\n",
        "    image = dataset_val.load_image(image_id)\n",
        "    results = model.detect([image], verbose=0)\n",
        "    r = results[0]\n",
        "    visualize.save_image(image = image[:,:,::-1], image_name=image_id, boxes=r['rois'], masks=r['masks'], class_ids=r['class_ids'], class_names=class_names, scores=r['scores'], save_dir=SAVE_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGnyg_JypZGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = ['Bench', 'Billboard', 'Catch Basin', 'CCTV Camera', 'Fire Hydrant', 'Junction Box', 'Mailbox',\n",
        "               'Manhole', 'Phone Booth', 'Street Light', 'Pole', 'Traffic Sign Frame', 'Utility Pole',\n",
        "               'Traffic Light', 'Traffic Sign (Back)', 'Traffic Sign (Front)', 'Trash Can']\n",
        "print(dataset_val.class_names[1:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHgMrQYDBpVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(APs)):\n",
        "    print(\"class name: {}: mAP: {}\".format(class_names[i], APs[i]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itQiOK_GOR0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_batch_ap_classwise_thresh(image_ids, class_id, thresh):\n",
        "    mPs = 0\n",
        "    numPs = 0\n",
        "    for image_id in image_ids:\n",
        "        # Load image\n",
        "        image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "            modellib.load_image_gt(dataset_val, config,\n",
        "                                  image_id, use_mini_mask=False)\n",
        "        # Run object detection\n",
        "        results = model.detect([image], verbose=0)\n",
        "        # Compute AP\n",
        "        r = results[0]\n",
        "        # visualize.display_instances(image, boxes=r['rois'], masks=r['masks'], class_ids=r['class_ids'], class_names=dataset_val.class_names[1:], scores=r['scores'])\n",
        "        # visualize.display_instances(image, boxes=gt_bbox, masks=gt_mask, class_ids=gt_class_id, class_names=dataset_val.class_names[1:])\n",
        "\n",
        "    \n",
        "        # Select one class\n",
        "        mask = np.where(gt_class_id == class_id, True, False)\n",
        "        gt_class_id_sel = gt_class_id[mask]\n",
        "\n",
        "        gt_bbox_sel = gt_bbox[mask,:]\n",
        "        gt_mask_sel = gt_mask[:,:,mask]\n",
        "                        \n",
        "        # Select class and scores higher than threshold\n",
        "        mask = np.where(np.logical_and(r['class_ids'] == class_id, r['scores'] > thresh), True, False)\n",
        "        r_rois_sel = r['rois'][mask,:]\n",
        "        r_class_ids_sel = r['class_ids'][mask]\n",
        "        r_scores_sel = r['scores'][mask]\n",
        "        r_masks_sel = r['masks'][:,:,mask] \n",
        "        if len(r_class_ids_sel) != 0:\n",
        "            AP, precisions, recalls, overlaps =\\\n",
        "                utils.compute_ap(gt_bbox_sel, gt_class_id_sel, gt_mask_sel,\n",
        "                                  r_rois_sel, r_class_ids_sel, r_scores_sel, r_masks_sel)\n",
        "            mPs = mPs + precisions[-2]\n",
        "            numPs += 1\n",
        "\n",
        "    print(\"numPs: {}\".format(numPs))\n",
        "    mPs = mPs / numPs\n",
        "    return mPs"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}