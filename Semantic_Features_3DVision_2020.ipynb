{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Semantic_Features_3DVision_2020.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juliuserbach/Semantic-Features/blob/master/Semantic_Features_3DVision_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATz3RlKSSsjx",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KERvIJ-nkskO",
        "colab_type": "text"
      },
      "source": [
        "Our mapping pipeline is of the following structure:\n",
        "\n",
        "1.   Detection of objects of certain object classes (e.g. traffic sign). Output: object bounding boxes\n",
        "2.   Triangulation of objects. Output: object position relative to pose\n",
        "3.   Creating map of objects. (And refining with filter, BA, etc.) Output: List of objects and their corresponding positions\n",
        "4.   Visualizing map.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1TSOFcEnsjx",
        "colab_type": "text"
      },
      "source": [
        "We start by loading the benchmark dataset. The Cityscapes dataset is used. \n",
        "\n",
        "Scripts for analyzing the dataset can be found here: https://github.com/mcordts/cityscapesScripts\n",
        "\n",
        "How to download the zip files directly: https://towardsdatascience.com/download-city-scapes-dataset-with-script-3061f87b20d7\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6ZU2SJr1Ifg",
        "colab_type": "text"
      },
      "source": [
        "Download CityScape files: \n",
        "\n",
        "*   leftImg8bit_trainextra.zip\n",
        "*   disparity_trainextra.zip\n",
        "*   camera_trainextra.zip\n",
        "*   vehicle_trainextra.zip\n",
        "\n",
        "The files are unzipped into data/... respectively.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwU_f_b7nH-O",
        "colab_type": "text"
      },
      "source": [
        "We start with the detection of objects of interest. Faster R-CNN is used for this task. (Julius)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX732O_UCkoo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# COCO Class names\n",
        "# Index of the class in the list is its ID. For example, to get ID of\n",
        "# the teddy bear class, use: class_names.index('teddy bear')\n",
        "class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
        "               'bus', 'train', 'truck', 'boat', 'traffic light',\n",
        "               'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',\n",
        "               'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',\n",
        "               'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
        "               'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "               'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "               'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
        "               'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
        "               'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
        "               'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n",
        "               'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "               'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
        "               'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "               'teddy bear', 'hair drier', 'toothbrush']\n",
        "static_classes = []              "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXh-WMRDzqFe",
        "colab_type": "text"
      },
      "source": [
        "Loading an on COCO pre-trained Faster R-CNN \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO8ZQm5lzz5s",
        "colab_type": "text"
      },
      "source": [
        "Installing Seamless Scene Segmentation from https://github.com/mapillary/seamseg\n",
        "For Seamless Scene Segmentation a pre-trained version on the Mapillary Dataset exists. Dataloaders for Cityscapes seem to exist, too.\n",
        "\n",
        "It is a network for panoptic segmentation basen on Mask-R-CNN. Alternatively the panoptic variant in the detectron2 reopisitory can also be trained on Cityscapes or Mapillary. Dataloaders seem to exist for both Datasets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLq9ofO5zzmS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "#!pip install git+https://github.com/mapillary/seamseg.git\n",
        "#!pip install wget\n",
        "\n",
        "#url = 'https://drive.google.com/file/d/1ULhd_CZ24L8FnI9lZ2H6Xuf03n6NA_-Y/view'\n",
        "#!wget $url\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bjqhrjt8BTs",
        "colab_type": "text"
      },
      "source": [
        "These functions convert a difference in coordinates (lat., long.) to a difference in metric frame (x, y) and vice versa. Both conversions are dependent on the current latitude.\n",
        "Under the assumption that we are in Europe, east of 0Â°!\n",
        "\n",
        "Approximative conversions:\n",
        "\n",
        "* Latitude: 1 deg = 110.574 km\n",
        "* Longitude: 1 deg = 111.320*cos(latitude) km\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2l9jM1A7_9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Felix\n",
        "# Helper functions for transformations.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Calculates the difference between two coordinates in meters (x to east, y to north).\n",
        "# See below for example.\n",
        "def coord_diff_to_metric_diff(d_lat, d_long, lat):\n",
        "  d_y = d_lat * 110574\n",
        "  d_x = d_long * 111320 * np.cos(np.radians(lat))\n",
        "  return d_x, d_y\n",
        "\n",
        "\n",
        "# Converts the metric difference to differences in latitude and longitude.\n",
        "# See below for example.\n",
        "def metric_diff_to_coord_diff(d_x, d_y, lat):\n",
        "  d_lat = d_y / 110574\n",
        "  d_long = d_x / 111320 / np.cos(np.radians(lat))\n",
        "  return d_lat, d_long\n",
        "\n",
        "\n",
        "# Returns the transformation matrix: world frame (-> vehicle frame) -> camera frame\n",
        "# Usage: [x_world; 1] = camera_frame_to_world_transform(args).dot([x_camera_frame; 1])\n",
        "# All angles in radians, distances in meters!\n",
        "# See below for example.\n",
        "def camera_frame_to_world_transform(heading, yaw_ext, pitch_ext, roll_ext, x_ext, y_ext, z_ext):\n",
        "    # Coordinate axis switch for camera.\n",
        "    C_c = np.array([[0., -1.,  0., 0.],\n",
        "                    [0.,  0., -1., 0.],\n",
        "                    [1.,  0.,  0., 0.],\n",
        "                    [0.,  0.,  0., 1.]])\n",
        "    # Vehicle to camera transformation matrix.\n",
        "    c_y = np.cos(yaw_ext)\n",
        "    c_p = np.cos(pitch_ext)\n",
        "    c_r = np.cos(roll_ext)\n",
        "    s_y = np.sin(yaw_ext)\n",
        "    s_p = np.sin(pitch_ext)\n",
        "    s_r = np.sin(roll_ext)\n",
        "    T_v_c = np.array([[c_y*c_p, c_y*s_p*s_r-s_y*c_r, c_y*s_p*c_r+s_y*s_r, x_ext],\n",
        "                      [s_y*c_p, s_y*s_p*s_r+c_y*c_r, s_y*s_p*c_r-c_y*s_r, y_ext],\n",
        "                      [   -s_p,             c_p*s_r,             c_p*c_r, z_ext],\n",
        "                      [     0.,                  0.,                  0.,    1.]])\n",
        "\n",
        "    # World to vehicle transformation matrix.\n",
        "    c_h = np.cos(heading)\n",
        "    s_h = np.sin(heading)\n",
        "    T_w_v = np.array([[c_h, -s_h, 0., 0.],\n",
        "                      [s_h,  c_h, 0., 0.],\n",
        "                      [ 0.,   0., 1., 0.],\n",
        "                      [ 0.,   0., 0., 1.]])\n",
        "\n",
        "    # Return camera space to vehicle transform.\n",
        "    return T_w_v.dot(T_v_c.dot(np.linalg.inv(C_c)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKWwzyVW_ZHa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "d7f9cd55-acb5-4dcf-dd3c-760652d244b4"
      },
      "source": [
        "# Testing and example (taken from Johannes' map pic on WhatsApp):\n",
        "# Converting coords into metrics:\n",
        "d_x, d_y = coord_diff_to_metric_diff(48.370-48.369, 10.896-10.894, 48.368)\n",
        "print(\"Delta x in meters: {}\".format(d_x))\n",
        "print(\"Delta y in meters: {}\".format(d_y))\n",
        "\n",
        "# Converting back to coords: should be the same:\n",
        "d_lat, d_long = metric_diff_to_coord_diff(d_x, d_y, 48.368)\n",
        "print(\"Difference in latitude: {}\".format(d_lat))\n",
        "print(\"Difference in longitude: {}\".format(d_long))\n",
        "\n",
        "# Convert a camera frame coordinate to world coordinate:\n",
        "x_example = np.array([[8.1], [0.7], [0.7]])\n",
        "x_world_example = camera_frame_to_world_transform(0.25, 0.0057, 0.055, 0.0, 1.7, -0.1, 1.3).dot(np.vstack((x_example, 1.0)))\n",
        "print(\"Camera coordinate: \")\n",
        "print(x_example)\n",
        "print(\"World coordinate: \")\n",
        "print(x_world_example)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Delta x in meters: 147.90949435332325\n",
            "Delta y in meters: 110.5739999997423\n",
            "Difference in latitude: 0.0009999999999976694\n",
            "Difference in longitude: 0.002000000000000668\n",
            "Camera coordinate: \n",
            "[[8.1]\n",
            " [0.7]\n",
            " [0.7]]\n",
            "World coordinate: \n",
            "[[ 4.35955249]\n",
            " [-7.34589949]\n",
            " [ 0.56257789]\n",
            " [ 1.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crfISE18wU_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mapping Algorithm Helper Functions\n",
        "\n",
        "import math\n",
        "\n",
        "# Convert degree to radian\n",
        "def degree_to_rad_converter(degree_value):\n",
        "  rad_value = degree_value * math.pi / 180\n",
        "  return rad_value\n",
        "\n",
        "# Calculate real 3D position\n",
        "def pixel_to_3Dposition_converter(x, depth, focal):\n",
        "  x_3D = x * depth / focal\n",
        "  return x_3D\n",
        "\n",
        "# Calculate box center to get average disparity\n",
        "def boundaries_to_box_center(x_1, x_2):\n",
        "  x_center = (x_1 + x_2)/2\n",
        "  return x_center\n",
        "\n",
        "# Get camera focal length and baseline (To be corrected, pixels are NOT quadratic here fx =/= fy)\n",
        "def camera_parameters(fx, fy):\n",
        "  # Take average, which is Bullshit, but not better idea yet\n",
        "  # focal = (fx + fy)/2\n",
        "  focal = fx\n",
        "  # Get baseline value from calibration data\n",
        "  baseline = 0.222384\n",
        "  return focal, baseline\n",
        "\n",
        "# Get disparities of all points (pixels)\n",
        "def disparity_catcher(x, y, img):\n",
        "  # DATA_DIR=\"data/disparity/train_extra/schweinfurt\"\n",
        "  # im_directory = list(glob.iglob(os.path.join(DATA_DIR, '*.*')))\n",
        "  # n_im = len(im_directory)\n",
        "\n",
        "\n",
        "  disp = img[y, x]\n",
        "  if disp != 0 and disp > 5125:\n",
        "    disp = (disp - 1.) / 256.\n",
        "\n",
        "    # print(disp)\n",
        "    return disp\n",
        "  else:\n",
        "    return \"nan\"\n",
        "\n",
        "\n",
        "# Convert disparity to actual depth information\n",
        "def disparity_to_depth(disparity_box_center):\n",
        "  if disparity_box_center > 0:\n",
        "    depth_center = baseline * focal / disparity_box_center\n",
        "  else:\n",
        "    depth_center = 100\n",
        "  return depth_center\n",
        "\n",
        "# Transformation to world frame coordinates\n",
        "# _3dImage\t=\tcv.reprojectImageTo3D('data/disparity/train_extra/schweinfurt/schweinfurt_000000_000000_disparity.png', Q)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaGRTzSON66G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get GPS Data from json files\n",
        "import pandas as pd \n",
        "import os\n",
        "import json\n",
        "import glob\n",
        "from pandas.io.json import json_normalize\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DATA_DIR=\"data/vehicle/train_extra/schweinfurt\"\n",
        "vehicle_datas_path = list(glob.iglob(os.path.join(DATA_DIR, '*.*')))\n",
        "n_images = len(vehicle_datas_path)\n",
        "vehicle_data_path_list = []\n",
        "images_path = list(glob.iglob(os.path.join(DATA_DIR, '*.*')))\n",
        "n_images = len(images_path)\n",
        "\n",
        "r = 0\n",
        "i = 0\n",
        "\n",
        "for r in range(n_images):\n",
        "    vehicle_data_path_list.append('data/vehicle/train_extra/schweinfurt/schweinfurt_000000_000'+ str(f\"{r:0>3}\") + '_vehicle.json')\n",
        "\n",
        "# create space for long at lat\n",
        "gpsLatitude_actual = np.zeros(n_images)\n",
        "gpsLongitude_actual = np.zeros(n_images)\n",
        "gpsHeading_actual = np.zeros(n_images)\n",
        "gpsLatitude_delta = np.zeros(n_images)\n",
        "gpsLongitude_delta = np.zeros(n_images)\n",
        "gpsHeading_delta = np.zeros(n_images)\n",
        "gpsHeading_actual_rad = np.zeros(n_images)\n",
        "\n",
        "\n",
        "for vehicle_data in vehicle_data_path_list:\n",
        "  # Load json from path\n",
        "  with open(vehicle_data) as json_file:\n",
        "    data = json.load(json_file)\n",
        "    gpsLatitude_actual[i] = data['gpsLatitude']\n",
        "    gpsLongitude_actual[i] = data['gpsLongitude']\n",
        "    gpsHeading_actual[i] = data['gpsHeading']\n",
        "    gpsHeading_actual_rad[i] = degree_to_rad_converter(gpsHeading_actual[i])\n",
        "    if i > 0:\n",
        "      gpsLatitude_delta[i] = gpsLatitude_actual[i] - gpsLatitude_actual[i-1]\n",
        "      gpsLongitude_delta[i] = gpsLongitude_actual[i] - gpsLongitude_actual[i-1]\n",
        "      gpsHeading_delta[i] = gpsHeading_actual[i] - gpsHeading_actual[i-1]\n",
        "  i+=1\n",
        "\n",
        "# Actual coordinates in x, y starting at 0, 0\n",
        "\n",
        "gps_step_delta_X = np.zeros(n_images)\n",
        "gps_total_delta_X = np.zeros(n_images)\n",
        "gps_step_delta_Y = np.zeros(n_images)\n",
        "gps_total_delta_Y = np.zeros(n_images)\n",
        "gps_total_delta_Heading = np.zeros(n_images)\n",
        "\n",
        "i = 0\n",
        "\n",
        "for image_path in images_path:\n",
        "  if i > 0:\n",
        "    gps_step_delta_X[i], gps_step_delta_Y[i] = coord_diff_to_metric_diff(gpsLatitude_delta[i], gpsLongitude_delta[i], gpsLatitude_actual[i])\n",
        "    gps_total_delta_X[i] = gps_total_delta_X[i-1] + gps_step_delta_X[i]\n",
        "    gps_total_delta_Y[i] = gps_total_delta_Y[i-1] + gps_step_delta_Y[i]\n",
        "    gps_total_delta_Heading[i] = gps_total_delta_Heading[i-1] + gpsHeading_delta[i]\n",
        "  i+=1\n",
        "\n",
        "i = 0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jjtUpajvkQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Camera Calibration Data\n",
        "\n",
        "DATA_DIR=\"data/camera/train_extra/schweinfurt\"\n",
        "cam_datas_path = list(glob.iglob(os.path.join(DATA_DIR, '*.*')))\n",
        "n_images = len(cam_datas_path)\n",
        "cam_data_path_list = []\n",
        "\n",
        "r = 0\n",
        "i = 0\n",
        "\n",
        "for i in range(n_images):\n",
        "    cam_data_path_list.append('data/camera/train_extra/schweinfurt/schweinfurt_000000_000'+ str(f\"{i:0>3}\") + '_camera.json')\n",
        "\n",
        "# create space for long at lat\n",
        "roll  = np.zeros(n_images)\n",
        "pitch = np.zeros(n_images)\n",
        "yaw = np.zeros(n_images)\n",
        "baseline = np.zeros(n_images)\n",
        "offset_x = np.zeros(n_images)\n",
        "offset_y = np.zeros(n_images)\n",
        "offset_z = np.zeros(n_images)\n",
        "f_x = np.zeros(n_images)\n",
        "f_y = np.zeros(n_images)\n",
        "u_0 = np.zeros(n_images)\n",
        "v_0 = np.zeros(n_images)\n",
        "\n",
        "for cam_data_path in cam_data_path_list:\n",
        "  # Load json from path\n",
        "  with open(cam_data_path) as json_file:\n",
        "    data = json.load(json_file)\n",
        "    roll[r] = data['extrinsic']['roll']\n",
        "    pitch[r] = data['extrinsic']['pitch']\n",
        "    yaw[r] = data['extrinsic']['yaw']\n",
        "    baseline[r] = data['extrinsic']['baseline']\n",
        "    offset_x[r] = data['extrinsic']['x']\n",
        "    offset_y[r] = data['extrinsic']['y']\n",
        "    offset_z[r] = data['extrinsic']['z']\n",
        "    f_x[r] = data['intrinsic']['fx']\n",
        "    f_y[r] = data['intrinsic']['fy']\n",
        "    u_0[r] = data['intrinsic']['u0']\n",
        "    v_0[r] = data['intrinsic']['v0']\n",
        "  r+=1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc2g5SVyrN2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Results File from Julius (Run only once !)\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "!wget 'https://raw.githubusercontent.com/juliuserbach/Semantic-Features/master/results.json' -O results.json\n",
        "\n",
        "\n",
        "path = \"data_KITTI\"\n",
        "os.mkdir(path)\n",
        "path = \"data_KITTI/Numerical_Results\"\n",
        "os.mkdir(path)\n",
        "shutil.copy(\"results.json\", \"data_KITTI/Numerical_Results\")\n",
        "\n",
        "# Download and unzip (run only once!).\n",
        "PATH_TO_CALIB = \"dataset/sequences/\"\n",
        "PATH_TO_POSE = \"dataset/poses/\"\n",
        "\n",
        "!wget \"https://onedrive.live.com/download?cid=EA356294C6263A37&resid=EA356294C6263A37%21100433&authkey=APrMUGQyaB4np4Q\" -O kitti_dataset.zip\n",
        "\n",
        "!mkdir data_KITTI\n",
        "!unzip -qq kitti_dataset.zip \"$PATH_TO_CALIB\"* -d data_KITTI\n",
        "!unzip -qq kitti_dataset.zip \"$PATH_TO_POSE\"* -d data_KITTI\n",
        "\n",
        "# And delete.\n",
        "!rm kitti_dataset.zip\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chKg6l15xzz0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "db0ae79d-c109-4646-be99-74d5182bff9d"
      },
      "source": [
        "# Mapping Algorithm -> Associates objects with respective coordinates in world coordinate frame and stores them in vector containing Long, Lat,\n",
        "#                      Height, class and corresponding image ID\n",
        "\n",
        "import pandas as pd\n",
        "import math\n",
        "import csv\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import glob\n",
        "\n",
        "# Ask which disparity version to take from disparity matrix. Options are: 0 for median or 1 for maximum\n",
        "\n",
        "option = input(\"Enter your option (0 = median, 1 = maximum): \")\n",
        "option = int(option)\n",
        "\n",
        "k = 0 # Images\n",
        "t = 0 # Total valid object number\n",
        "\n",
        "# Get focal length from calibration data\n",
        "fx = 2273.82\n",
        "fy = 2236.9989965173254\n",
        "# baseline = 0.222384\n",
        "\n",
        "focal, baseline = camera_parameters(fx, fy)\n",
        "\n",
        "\n",
        "x_3D_rel_car = []\n",
        "y_3D_rel_car = []\n",
        "z_3D_rel_car = []\n",
        "gpslat = []\n",
        "gpslong = []\n",
        "relz = []\n",
        "semanticlabel = []\n",
        "imageid = []\n",
        "\n",
        "results_directory = '/content/data_KITTI/Numerical_Results/results.json'\n",
        "\n",
        "# Load json from path\n",
        "with open(results_directory) as json_file:\n",
        "  data = json.load(json_file)\n",
        "  for i in range(len(data['results'])):\n",
        "    print(\"Processing image number\", k)\n",
        "    print(x_3D_rel_car)\n",
        "    n_objects = int(len(data['results'][k]['classes']))\n",
        "    # Get image number\n",
        "    image_no = data['results'][k]['image_id'][3:6]\n",
        "    print(image_no)\n",
        "    im_no = int(image_no)\n",
        "    # disparity_data_path = 'data/disparity/train_extra/schweinfurt/schweinfurt_000000_000'+ image_no + '_disparity.png'\n",
        "    disparity_data_path = 'data_KITTI/dataset/sequences/04/disparity_2/' + image_no + 'raw.png'\n",
        "\n",
        "    # Load image from path\n",
        "    img = cv2.imread(disparity_data_path, cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "    for o in range(n_objects):\n",
        "      # Get coordinates in image frame\n",
        "      u1 = data['results'][k]['boxes'][o][0]\n",
        "      v1 = data['results'][k]['boxes'][o][1]\n",
        "      u2 = data['results'][k]['boxes'][o][2]\n",
        "      v2 = data['results'][k]['boxes'][o][3]\n",
        "\n",
        "      # Calculate center point in image frame\n",
        "      u_cent = int(boundaries_to_box_center(u1, u2))\n",
        "      v_cent = int(boundaries_to_box_center(v1, v2))\n",
        "\n",
        "      # Get and write all disparities 1/10th of box-length around center point in matrix\n",
        "      disp_matrix = np.zeros(shape=(int(abs((u_cent-u1)/1)),int(abs((v_cent-v1)/1))))\n",
        "        \n",
        "      for y_coord in range(int(abs((v_cent-v1)/1))):\n",
        "        for x_coord in range(int(abs((u_cent-u1)/1))):\n",
        "          #print(disparity_catcher(x_coord + int(u_cent) - int(abs(u_cent - u1)/10), y_coord + int(v_cent) - int(abs(v_cent - v1)/10), img))\n",
        "          disp_matrix[x_coord][y_coord] = disparity_catcher(x_coord + int(u_cent) - int(abs(u_cent - u1)/2), y_coord + int(v_cent) - int(abs(v_cent - v1)/2), img)\n",
        "        \n",
        "      # Take median of disparity values\n",
        "      \n",
        "      if option == 0:\n",
        "        med_disp = np.nanmedian(disp_matrix, axis=None, out=None, overwrite_input=False, keepdims=False)\n",
        "      if option == 1:\n",
        "        try:\n",
        "          med_disp = np.nanmax(disp_matrix, axis=None, out=None, keepdims=False)\n",
        "        except RuntimeWarning:\n",
        "          med_disp = np.nanmax(disp_matrix, axis=None, out=None, keepdims=False)\n",
        "        #else:\n",
        "          #med_disp = np.nanmax(disp_matrix, axis=None, out=None, keepdims=False)\n",
        "\n",
        "      if med_disp != 0:\n",
        "\n",
        "        # Get disparity of center point in image frame\n",
        "        # disp_cent = disparity_catcher(u_cent, v_cent, k)\n",
        "\n",
        "        # Calculate depth from disparity\n",
        "        depth_med = disparity_to_depth(med_disp)\n",
        "\n",
        "        # Transform coordinates from image plane to camera frame\n",
        "        x1 = depth_med * (u1 - u_0[im_no]) / f_x[im_no]\n",
        "        y1 = depth_med * (v1 - v_0[im_no]) / f_y[im_no]\n",
        "        x2 = depth_med * (u2 - u_0[im_no]) / f_x[im_no]\n",
        "        y2 = depth_med * (v2 - v_0[im_no]) / f_y[im_no]\n",
        "\n",
        "        # Calculate box center in camera frame\n",
        "        x_cent = boundaries_to_box_center(x1, x2)\n",
        "        y_cent = boundaries_to_box_center(y1, y2)\n",
        "\n",
        "        # Save object coordinates in camera frame\n",
        "        x_3D_rel_car.append(x_cent)\n",
        "        y_3D_rel_car.append(y_cent)\n",
        "        z_3D_rel_car.append(depth_med)\n",
        "\n",
        "        # Transform object coordinates to world frame\n",
        "        input_vector = np.array([[x_3D_rel_car[t]], [y_3D_rel_car[t]], [z_3D_rel_car[t]]])\n",
        "        output_vector = camera_frame_to_world_transform(gpsHeading_actual_rad[im_no], yaw[im_no], pitch[im_no], roll[im_no], offset_x[im_no], offset_y[im_no], offset_z[im_no]).dot(np.vstack((input_vector, 1.0)))\n",
        "        \n",
        "        # Transform back to Lat / Long and save to vector\n",
        "        new_x = -output_vector[1]\n",
        "        new_y = output_vector[0]\n",
        "        d_lat, d_long = metric_diff_to_coord_diff(new_x, new_y, gpsLatitude_actual[im_no])\n",
        "        \n",
        "        #dataframe\n",
        "        gpslat.append(gpsLatitude_actual[im_no] + d_lat)\n",
        "        gpslong.append(gpsLongitude_actual[im_no] + d_long)\n",
        "        relz.append(output_vector[2])\n",
        "        semanticlabel.append(data['results'][k]['classes'][o])\n",
        "        imageid.append(data['results'][k]['image_id'][3:6])\n",
        "      \n",
        "        t += 1\n",
        "    k += 1\n",
        "\n",
        "  mapping_output_df = pd.DataFrame({'imageID': imageid, 'longitude': gpslong, 'latitude': gpslat, 'z': relz, 'semanticlabel': semanticlabel})\n",
        "\n",
        "  for i in mapping_output_df:\n",
        "    mapping_output_df[i] = np.squeeze(mapping_output_df[i].tolist())\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter your option (0 = median, 1 = maximum): 1\n",
            "Processing image number 0\n",
            "[]\n",
            "163\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-aeb7e47c7308>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx_coord\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_cent\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mu1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m           \u001b[0;31m#print(disparity_catcher(x_coord + int(u_cent) - int(abs(u_cent - u1)/10), y_coord + int(v_cent) - int(abs(v_cent - v1)/10), img))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m           \u001b[0mdisp_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_coord\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_coord\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisparity_catcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_coord\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_cent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_cent\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mu1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_coord\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_cent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_cent\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m       \u001b[0;31m# Take median of disparity values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-62fd47941f6a>\u001b[0m in \u001b[0;36mdisparity_catcher\u001b[0;34m(x, y, img)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m   \u001b[0mdisp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdisp\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdisp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m5125\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mdisp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdisp\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m256.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 390 is out of bounds for axis 0 with size 370"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jII6DrcXfVHI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Writes Data to DataFrame and to csv file\n",
        "\n",
        "# Create DataFrame \n",
        "\n",
        "\n",
        "# Print the output\n",
        "print(mapping_output_df)\n",
        "\n",
        "# Write output to csv file\n",
        "mapping_output_df.to_csv(r'data/mapping_output.csv', index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7Rt99lHkDje",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "60230a08-7f87-483e-8642-186e9302687e"
      },
      "source": [
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "!pip install git+https://github.com/utiasSTARS/pykitti.git\n",
        "\n",
        "import pykitti\n",
        "\n",
        "# Change this to the directory where you store KITTI data\n",
        "basedir = '/content/data_KITTI/dataset'\n",
        "\n",
        "# Specify the dataset to load\n",
        "sequence = '04'\n",
        "\n",
        "# Load the data. Optionally, specify the frame range to load.\n",
        "# dataset = pykitti.odometry(basedir, sequence)\n",
        "dataset = pykitti.odometry(basedir, sequence, frames=range(0, 20, 5))\n",
        "\n",
        "# dataset.calib:      Calibration data are accessible as a named tuple\n",
        "# dataset.timestamps: Timestamps are parsed into a list of timedelta objects\n",
        "# dataset.poses:      List of ground truth poses T_w_cam0\n",
        "# dataset.camN:       Generator to load individual images from camera N\n",
        "# dataset.gray:       Generator to load monochrome stereo pairs (cam0, cam1)\n",
        "# dataset.rgb:        Generator to load RGB stereo pairs (cam2, cam3)\n",
        "# dataset.velo:       Generator to load velodyne scans as [x,y,z,reflectance]\n",
        "\n",
        "# Grab some data\n",
        "first_pose = dataset.poses[0]\n",
        "second_pose = dataset.poses[1]\n",
        "\n",
        "print(first_pose)\n",
        "print(second_pose)\n",
        "\n",
        "# To do:\n",
        "# - Get Position of car\n",
        "# - Get Camera Calibration data of left camera\n",
        "# - Do normal camera calibration but with new data\n",
        "# - Get disparity using new file\n",
        "# - Add new position data onto coordinates\n",
        "\n",
        "\n",
        "# Load the data. Optionally, specify the frame range to load.\n",
        "# dataset = pykitti.odometry(basedir, sequence)\n",
        "dataset = pykitti.odometry(basedir, sequence)\n",
        "\n",
        "# Grab some data\n",
        "second_pose = dataset.poses[219]\n",
        "first_rgb = dataset.get_rgb(219)\n",
        "first_cam2 = dataset.get_cam2(219)\n",
        "velo = dataset.get_velo(219)\n",
        "baseline = dataset.calib.b_rgb\n",
        "\n",
        "disparity = cv2.imread(\"/home/felix/vision_ws/Semantic-Features/content/kitti_dataset/dataset/sequences/04/rawpng/219raw.png\",\n",
        "                           cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "P_cam2 = dataset.calib.P_rect_20\n",
        "T_cam2_velo = dataset.calib.T_cam2_velo\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/utiasSTARS/pykitti.git\n",
            "  Cloning https://github.com/utiasSTARS/pykitti.git to /tmp/pip-req-build-f67i6qyn\n",
            "  Running command git clone -q https://github.com/utiasSTARS/pykitti.git /tmp/pip-req-build-f67i6qyn\n",
            "Requirement already satisfied (use --upgrade to upgrade): pykitti==0.3.1 from git+https://github.com/utiasSTARS/pykitti.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pykitti==0.3.1) (1.18.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from pykitti==0.3.1) (3.2.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from pykitti==0.3.1) (7.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pykitti==0.3.1) (1.0.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pykitti==0.3.1) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pykitti==0.3.1) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pykitti==0.3.1) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pykitti==0.3.1) (1.2.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->pykitti==0.3.1) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->pykitti==0.3.1) (1.12.0)\n",
            "Building wheels for collected packages: pykitti\n",
            "  Building wheel for pykitti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pykitti: filename=pykitti-0.3.1-cp36-none-any.whl size=13234 sha256=a90fc85640e094639f8e94592b84a88c12dce5bfd672858c753baff6fd17e1f8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qe93mglb/wheels/4b/55/6f/2986578a5cf7afc0ea7e14cfff9cf5bc8e625e9c4e553f02f9\n",
            "Successfully built pykitti\n",
            "[[ 1.000000e+00  1.197625e-11  1.704638e-10 -5.551115e-17]\n",
            " [ 1.197625e-11  1.000000e+00  3.562503e-10  0.000000e+00]\n",
            " [ 1.704638e-10  3.562503e-10  1.000000e+00  2.220446e-16]\n",
            " [ 0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00]]\n",
            "[[ 9.999922e-01 -3.925219e-03 -5.162240e-04  4.157928e-03]\n",
            " [ 3.926062e-03  9.999909e-01  1.640787e-03 -1.014678e-01]\n",
            " [ 5.097792e-04 -1.642800e-03  9.999985e-01  6.581638e+00]\n",
            " [ 0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}