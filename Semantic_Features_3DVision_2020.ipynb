{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Semantic_Features_3DVision_2020.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juliuserbach/Semantic-Features/blob/master/Semantic_Features_3DVision_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KERvIJ-nkskO",
        "colab_type": "text"
      },
      "source": [
        "Our mapping pipeline is of the following structure:\n",
        "\n",
        "1.   Detection of objects of certain object classes (e.g. traffic sign). Output: object bounding boxes\n",
        "2.   Triangulation of objects. Output: object position relative to pose\n",
        "3.   Creating map of objects. (And refining with filter, BA, etc.) Output: List of objects and their corresponding positions\n",
        "4.   Visualizing map.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1TSOFcEnsjx",
        "colab_type": "text"
      },
      "source": [
        "We start by loading the benchmark dataset. The Cityscapes dataset is used. \n",
        "\n",
        "Scripts for analyzing the dataset can be found here: https://github.com/mcordts/cityscapesScripts\n",
        "\n",
        "How to download the zip files directly: https://towardsdatascience.com/download-city-scapes-dataset-with-script-3061f87b20d7\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6ZU2SJr1Ifg",
        "colab_type": "text"
      },
      "source": [
        "Download CityScape files: \n",
        "\n",
        "*   leftImg8bit_trainextra.zip\n",
        "*   disparity_trainextra.zip\n",
        "*   camera_trainextra.zip\n",
        "*   vehicle_trainextra.zip\n",
        "\n",
        "The files are unzipped into data/... respectively.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QM1GyIOSzP06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download and unzip (run only once!).\n",
        "trajectoryname = \"schweinfurt\"\n",
        "\n",
        "# Remove data directory if it is already loaded (if needed).\n",
        "#!rm -r data\n",
        "# Login.\n",
        "!wget --keep-session-cookies --save-cookies=cookies.txt --post-data 'username=ftaubner@ethz.ch&password=semantic_dudes&submit=Login' https://www.cityscapes-dataset.com/login/\n",
        "# Get left camera images. I put it on OneDrive so that only Schweinfurt has to be downloaded.\n",
        "!wget --no-check-certificate \"https://onedrive.live.com/download?cid=EA356294C6263A37&resid=EA356294C6263A37%2199734&authkey=AC5K24PFcrSPFl4\" -O leftImg8bit_trainextra.zip\n",
        "# Alternatively, it can be downloaded from CityScapes website directly:\n",
        "#   (!wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=4)\n",
        "# Extract.\n",
        "!mkdir data\n",
        "!unzip leftImg8bit_trainextra.zip 'leftImg8bit/train_extra/schweinfurt/*' -d data\n",
        "# And delete.\n",
        "!rm leftImg8bit_trainextra.zip\n",
        "\n",
        "# Get disparity maps.\n",
        "!wget --no-check-certificate \"https://onedrive.live.com/download?cid=EA356294C6263A37&resid=EA356294C6263A37%2199735&authkey=AEwGRlH_TySnyPM\" -O disparity_trainextra.zip\n",
        "# Original file:\n",
        "#   (!wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=22)\n",
        "!unzip disparity_trainextra.zip 'disparity/train_extra/schweinfurt/*' -d data\n",
        "!rm disparity_trainextra.zip\n",
        "# Get camera intrinsics. \n",
        "!wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=9\n",
        "!unzip camera_trainextra.zip 'camera/train_extra/schweinfurt/*' -d data\n",
        "!rm camera_trainextra.zip\n",
        "# Get vehicle odometry.\n",
        "!wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=11\n",
        "!unzip vehicle_trainextra.zip 'vehicle/train_extra/schweinfurt/*' -d data\n",
        "!rm vehicle_trainextra.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwU_f_b7nH-O",
        "colab_type": "text"
      },
      "source": [
        "We start with the detection of objects of interest. Faster R-CNN is used for this task. (Julius)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UEmUojHy0Zy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install dependencies: (use cu100 because colab is on CUDA 10.0)\n",
        "!pip install -U torch==1.4+cu100 torchvision==0.5+cu100 -f https://download.pytorch.org/whl/torch_stable.html \n",
        "!pip install cython pyyaml==5.1\n",
        "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "import torch, torchvision\n",
        "torch.__version__\n",
        "!gcc --version\n",
        "# opencv is pre-installed on colab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbkKU6cny-6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install detectron2:\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu100/index.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEnImM8jzDVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You may need to restart your runtime prior to this, to let your installation take effect\n",
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXh-WMRDzqFe",
        "colab_type": "text"
      },
      "source": [
        "Loading an on COCO pre-trained Faster R-CNN \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNOoJunDzH55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cfg = get_cfg()\n",
        "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
        "# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "im = cv2.imread(\"Test.jpeg\")\n",
        "plt.imshow(im)\n",
        "outputs = predictor(im)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yphk0gIdzvEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#visualize results\n",
        "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "plt.imshow(v.get_image()[:, :, :])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO8ZQm5lzz5s",
        "colab_type": "text"
      },
      "source": [
        "Installing Seamless Scene Segmentation from https://github.com/mapillary/seamseg\n",
        "For Seamless Scene Segmentation a pre-trained version on the Mapillary Dataset exists. Dataloaders for Cityscapes seem to exist, too.\n",
        "\n",
        "It is a network for panoptic segmentation basen on Mask-R-CNN. Alternatively the panoptic variant in the detectron2 reopisitory can also be trained on Cityscapes or Mapillary. Dataloaders seem to exist for both Datasets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLq9ofO5zzmS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "!pip install git+https://github.com/mapillary/seamseg.git\n",
        "!pip install wget\n",
        "\n",
        "url = 'https://drive.google.com/file/d/1ULhd_CZ24L8FnI9lZ2H6Xuf03n6NA_-Y/view'\n",
        "wget.download(url)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bjqhrjt8BTs",
        "colab_type": "text"
      },
      "source": [
        "These functions convert a difference in coordinates (lat., long.) to a difference in metric frame (x, y) and vice versa. Both conversions are dependent on the current latitude.\n",
        "Under the assumption that we are in Europe, east of 0°!\n",
        "\n",
        "Approximative conversions:\n",
        "\n",
        "* Latitude: 1 deg = 110.574 km\n",
        "* Longitude: 1 deg = 111.320*cos(latitude) km\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2l9jM1A7_9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Calculates the difference between two coordinates in meters (x to east, y to north).\n",
        "def coord_diff_to_metric_diff(d_lat, d_long, lat):\n",
        "  d_y = d_lat * 110574\n",
        "  d_x = d_long * 111320 * np.cos(np.radians(lat))\n",
        "  return d_x, d_y\n",
        "\n",
        "# Converts the metric difference to differences in latitude and longitude.\n",
        "def metric_diff_to_coord_diff(d_x, d_y, lat):\n",
        "  d_lat = d_y / 110574\n",
        "  d_long = d_x / 111320 / np.cos(np.radians(lat))\n",
        "  return d_lat, d_long"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKWwzyVW_ZHa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "23ccd7a1-f33f-4071-baf7-e339ef78d184"
      },
      "source": [
        "# Testing and example (taken from Johannes' map pic on WhatsApp):\n",
        "# Converting coords into metrics:\n",
        "d_x, d_y = coord_diff_to_metric_diff(48.370-48.369, 10.896-10.894, 48.368)\n",
        "print(\"Delta x in meters: {}\".format(d_x))\n",
        "print(\"Delta y in meters: {}\".format(d_y))\n",
        "\n",
        "# Converting back to coords: should be the same:\n",
        "d_lat, d_long = metric_diff_to_coord_diff(d_x, d_y, 48.368)\n",
        "print(\"Difference in latitude: {}\".format(d_lat))\n",
        "print(\"Difference in longitude: {}\".format(d_long))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Delta x in meters: 147.90949435332325\n",
            "Delta y in meters: 110.5739999997423\n",
            "Difference in latitude: 0.0009999999999976694\n",
            "Difference in longitude: 0.002000000000000668\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}